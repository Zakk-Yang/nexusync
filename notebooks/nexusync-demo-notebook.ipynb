{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NexuSync Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use NexuSync for document indexing, querying, and other key functionalities.\n",
    "\n",
    "NexuSync is a powerful library designed for efficient document indexing and querying, using state-of-the-art language and embedding models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NexuSync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing NexuSync with the required parameters. Adjust the following parameters according to your use case:\n",
    "\n",
    "- **input_dirs**: List of directories containing documents for indexing.\n",
    "- **openai_model_yn**: Whether to use OpenAI models for embeddings and language tasks.\n",
    "- **embedding_model**: Model to be used for generating embeddings.\n",
    "- **language_model**: Model to be used for language tasks.\n",
    "- **chroma_db_dir**: Directory for storing ChromaDB files.\n",
    "- **index_persist_dir**: Directory for persisting the index.\n",
    "- **chunk_size**: Size of the text chunks to be used for creating embeddings.\n",
    "- **chunk_overlap**: Overlap between text chunks to maintain context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zy-wsl/miniconda3/envs/nexusync/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-09 17:12:24,344 - nexusync.utils.embedding_models.set_embedding_model - INFO - Using HuggingFace embedding model: BAAI/bge-base-en-v1.5\n",
      "2024-10-09 17:12:24,348 - nexusync.utils.embedding_models.set_language_model - INFO - Using Ollama LLM model: llama3.2\n",
      "2024-10-09 17:12:24,429 - nexusync.NexuSync - INFO - Vectors and Querier initialized successfully.\n",
      "2024-10-09 17:12:24,430 - nexusync.core.indexer - WARNING - Index not found. Building a new index.\n",
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "2024-10-09 17:12:34,653 - nexusync.core.indexer - INFO - Loaded 40 chunks from ../sample_docs.\n",
      "2024-10-09 17:12:36,160 - nexusync.core.indexer - INFO - Index Built.\n"
     ]
    }
   ],
   "source": [
    "from nexusync import NexuSync\n",
    "\n",
    "OPENAI_MODEL_YN = False # if False, you will use ollama model\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\" # suggested embedding model\n",
    "LANGUAGE_MODEL = 'llama3.2' # you need to download ollama model first, please check https://ollama.com/download\n",
    "TEMPERATURE = 0.4 # range from 0 to 1, higher means higher creativitiy level\n",
    "CHROMA_DB_DIR = 'chroma_db'\n",
    "INDEX_PERSIST_DIR = 'index_storage'\n",
    "CHROMA_COLLECTION_NAME = 'my_collection'\n",
    "INPUT_DIRS = [\"../sample_docs\"] # can specify multiple document paths\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 20\n",
    "RECURSIVE = True\n",
    "\n",
    "ns = NexuSync(input_dirs=INPUT_DIRS, \n",
    "              openai_model_yn=False, \n",
    "              embedding_model=EMBEDDING_MODEL, \n",
    "              language_model=LANGUAGE_MODEL, \n",
    "              temperature=TEMPERATURE, \n",
    "              chroma_db_dir = CHROMA_DB_DIR,\n",
    "              index_persist_dir = INDEX_PERSIST_DIR,\n",
    "              chroma_collection_name=CHROMA_COLLECTION_NAME,\n",
    "              chunk_overlap=CHUNK_OVERLAP,\n",
    "              chunk_size=CHUNK_SIZE,\n",
    "              recursive=RECURSIVE\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:13:30,704 - nexusync.core.chat_engine - INFO - Chat engine initialized\n"
     ]
    }
   ],
   "source": [
    "text_qa_template = \"\"\"\n",
    "Context Information:\n",
    "--------------------\n",
    "{context_str}\n",
    "--------------------\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the context information and the query.\n",
    "2. Think through the problem step by step.\n",
    "3. Provide a concise and accurate answer based on the given context.\n",
    "4. If the answer cannot be determined from the context, state \"Based on the given information, I cannot provide a definitive answer.\"\n",
    "5. If you need to make any assumptions, clearly state them.\n",
    "6. If relevant, provide a brief explanation of your reasoning.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "ns.initialize_stream_chat(\n",
    "    text_qa_template=text_qa_template,\n",
    "    chat_mode=\"context\",\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, Nvidia's ecosystem appears to refer to their comprehensive suite of technologies, tools, and solutions for accelerating various workloads in the datacenter, particularly in areas such as artificial intelligence (AI), high-performance computing (HPC), and big data analytics.\n",
      "\n",
      "Nvidia's ecosystem includes:\n",
      "\n",
      "1. **GPUs**: Nvidia's line of graphics processing units (GPUs) that are designed for AI, HPC, and other compute-intensive applications.\n",
      "2. **GPUDirect Storage**: A software framework that enables direct storage access between GPUs and storage systems.\n",
      "3. **Nvidia Container Runtime**: A container runtime that provides a managed environment for running containers on Nvidia GPUs.\n",
      "4. **NGC Registry**: A container registry for Nvidia GPU-accelerated applications.\n",
      "5. **TensorFlow** and **PyTorch**: Popular deep learning frameworks that are supported by Nvidia's ecosystem.\n",
      "6. **CUDA**: Nvidia's parallel computing platform and programming model.\n",
      "\n",
      "Nvidia's ecosystem also includes a range of software tools, libraries, and frameworks that support AI, HPC, and big data analytics workloads, such as:\n",
      "\n",
      "1. **Deep Learning Software Development Kit (SDK)**: A set of tools and libraries for building deep learning applications.\n",
      "2. **TensorRT**: An open-source optimization tool for deep learning models.\n",
      "3. **Nvidia Deep Learning SDK**: A software development kit for building AI applications.\n",
      "\n",
      "Overall, Nvidia's ecosystem is designed to provide a comprehensive set of technologies and solutions for accelerating various workloads in the datacenter, particularly in areas such as AI, HPC, and big data analytics.\n",
      "\n",
      "Full response: Based on the context provided, Nvidia's ecosystem appears to refer to their comprehensive suite of technologies, tools, and solutions for accelerating various workloads in the datacenter, particularly in areas such as artificial intelligence (AI), high-performance computing (HPC), and big data analytics.\n",
      "\n",
      "Nvidia's ecosystem includes:\n",
      "\n",
      "1. **GPUs**: Nvidia's line of graphics processing units (GPUs) that are designed for AI, HPC, and other compute-intensive applications.\n",
      "2. **GPUDirect Storage**: A software framework that enables direct storage access between GPUs and storage systems.\n",
      "3. **Nvidia Container Runtime**: A container runtime that provides a managed environment for running containers on Nvidia GPUs.\n",
      "4. **NGC Registry**: A container registry for Nvidia GPU-accelerated applications.\n",
      "5. **TensorFlow** and **PyTorch**: Popular deep learning frameworks that are supported by Nvidia's ecosystem.\n",
      "6. **CUDA**: Nvidia's parallel computing platform and programming model.\n",
      "\n",
      "Nvidia's ecosystem also includes a range of software tools, libraries, and frameworks that support AI, HPC, and big data analytics workloads, such as:\n",
      "\n",
      "1. **Deep Learning Software Development Kit (SDK)**: A set of tools and libraries for building deep learning applications.\n",
      "2. **TensorRT**: An open-source optimization tool for deep learning models.\n",
      "3. **Nvidia Deep Learning SDK**: A software development kit for building AI applications.\n",
      "\n",
      "Overall, Nvidia's ecosystem is designed to provide a comprehensive set of technologies and solutions for accelerating various workloads in the datacenter, particularly in areas such as AI, HPC, and big data analytics.\n",
      "Metadata: {'sources': [{'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #0: \\nNvidia ecosystem\\x0band how Futurewei supports it\\n\\n\\n\\nSlide #1: \\nOptimization stack\\n\\n Image: a series of photos showing different types of green plants\\n\\n\\nMulti-core, multi-thread programming library that supports vector optimization. ML, DL libraries and tools based on CUDA. Bluefield DPU, MPI tag matching, Mellanox SHARP\\nTelemetry and troubleshooting across compute, network, and storage layers. Cumulus NetQ, Mellanox UFM\\nSource : https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/\\n\\n\\nSlide #3: \\nSoftware ecosystems from Nvidia\\nNGC registry\\nhttp://ngc.nvidia.com\\n\\nContainers, models, pipelines for Nvidia GPUs. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. ML, DL libraries and tools based on CUDA. APP/data engine/AIML pipeline\\nSource: https://www.nvidia.com/en-us/data-center/magnum-io/\\n\\n\\nSlide #2: \\nFUTUREWEI INTERNAL\\n3\\n\\n Image: a collage of photos showing different types of electronic devices\\n\\n\\n Image: a green and white sign on a blue wall\\n\\nThe GPU bypasses the CPU and system memory, and accesses remote storage via 8X 200 Gb/s NICs, achieving up to 1.6Terabits/s of raw storage bandwidth. GPUDirect, Mellanox NVMe SNAP\\nNVIDIA NVLink® fabric and RDMA-based network IO acceleration reduces IO overhead, bypassing the CPU and enabling direct GPU to GPU data transfers at line rates\\nDPDK, GPUDirect RDMA, HPC-X, NCCL, NVSHEM, UCX, ASAP\\nOffloading to “network processors”. APP/data engine/AIML pipeline\\nSource: https://www.nvidia.com/en-us/data-center/magnum-io/\\n\\n\\nSlide #2: \\nFUTUREWEI INTERNAL\\n3\\n\\n Image: a collage of photos showing different types of electronic devices\\n\\n\\n Image: a green and white sign on a blue wall\\n\\nThe GPU bypasses the CPU and system memory, and accesses remote storage via 8X 200 Gb/s NICs, achieving up to 1.6Terabits/s of raw storage bandwidth. GPUDirect, Mellanox NVMe SNAP\\nNVIDIA NVLink® fabric and RDMA-based network IO acceleration reduces IO overhead, bypassing the CPU and enabling direct GPU to GPU data transfers at line rates\\nDPDK, GPUDirect RDMA, HPC-X, NCCL, NVSHEM, UCX, ASAP\\nOffloading to “network processors”. Bluefield DPU, MPI tag matching, Mellanox SHARP\\nTelemetry and troubleshooting across compute, network, and storage layers. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. Slide #5: \\nTarget #2: GPU-heavy cluster (DGX-2 like) storage reference architecture (less dependent, less expensive)\\nFUTUREWEI INTERNAL\\n6\\n…\\nServers loaded with Tesla GPUs\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv Cumulus NetQ, Mellanox UFM\\nSource : https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/\\n\\n\\nSlide #3: \\nSoftware ecosystems from Nvidia\\nNGC registry\\nhttp://ngc.nvidia.com\\n\\nContainers, models, pipelines for Nvidia GPUs. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. Slide #5: \\nTarget #2: GPU-heavy cluster (DGX-2 like) storage reference architecture (less dependent, less expensive)\\nFUTUREWEI INTERNAL\\n6\\n…\\nServers loaded with Tesla GPUs\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nImage: a black and white photo of a black and white tv\\n\\n\\n\\nSlide #6: \\nCommon data solution reference architecture (open-source)\\nFUTUREWEI INTERNAL\\n7\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost 1\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost n\\n\\nMulti-host connected by high speed RDMA network (IB/RoCE)\\nNIC\\nNIC\\n\\nRemote storage\\nNIC\\n\\nConnect to storage with NVMe over Fabric (IB/RoCE)\\nLUNs\\nFiles\\nObjects\\nNVMe driver\\nCUDA\\nData lake\\nAI\\nADAS\\nCommon\\nPython3\\nCUDA-X\\nDocker\\nK8S\\nJava\\nGCC\\nScala\\nSpark core\\nSystem\\nConfiguration\\nSoftware\\nConfiguration\\nTensorFlow\\nPytorch\\nSpark SQL\\nHive\\nPresto\\nAlluxio\\nTPC-DS\\nBigBench v2\\nBDD 100K\\nSpark ML\\nFlink\\nSpark Stream\\nHorovod\\nRay\\nGPU driver\\nJupyter hub\\nApollo\\nCARLA\\nErdos\\nAWS\\nAzure\\nGoogle Cloud\\nHPC\\nopenHPC\\ntest-suite-ohpc\\nMySQL\\nKeras\\nNvidia container runtime\\nGPU scheduler? Prometheus\\nGrafana\\nKafka\\nArgo workflow\\nGPU monitoring\\nVertical (ADAS, BIO) training\\nSimulation\\nNFS\\n\\nExtended solution\\n\\n\\n\\nSlide #7: \\nBenchmark selection/adoption\\nNeed to have storage needs (large data sets, IO bound, etc)\\nCandidates\\nBDD100k (ADAS related) (2TB)\\nSimulation and visualizing NASA Mars Lander data set (128TB)\\nSimulate, analyze, and visualize molecular dynamics dataset (17TB)\\nTPCx-BB express benchmark BB (Hadoop based big data system), IO intensity queries. Later on we can look into pros and cons. Source: Webinar: NVIDIA GPUDirect Storage: Accelerating the Data Path to the GPU\\nhttps://info.nvidia.com/gpudirect-storage-webinar-reg-page.html?thankyou=true&aliId=eyJpIjoiNHNZQVdNQkhoRWs3UE1zOCIsInQiOiJxWnRJWitnWUhLUFBWVjFOZW1EdlZRPT0ifQ%253D%253D&ondemandrgt=yes\\n\\nhttps://on24static.akamaized.net/event/24/01/05/4/rt/1/documents/resourceList1594683598315/gdsq320webinar1594683596098.pdf\\n\\nReference numbers in the industry:\\n\\nDGX-2 Non-GDS: <40GB/s with high CPU util. rate\\nDGX-2 GDS: 53GB/s with 16 drives\\nWekaIO GDS: >80GB/s\\nVast GDS: >90GB/s peak\\nSource: https://blocksandfiles.com/2020/10/12/wekaio-nvidia-gpudirect-benchmark/\\nhttps://blocksandfiles.com/2020/07/23/nvidia-gpudirect-storage-software/ TPC-H\\nGDS for PyTorch (GDS numpy, GDS numpy DALI, GDS numpy 1-file with DeepLabv3+), on CAM5 climate data set to predict extreme weather patterns. FUTUREWEI INTERNAL\\n8\\n\\n\\nSlide #8: \\nNvidia GPUDirect Storage (GDS) support, v0.9 Nov 2020\\nFUTUREWEI INTERNAL\\n9\\nMode 1: “NVMeoF with ext4”\\nTesla GPU\\n\\nlibcufile.so\\nUser space\\n\\n\\nNvidia-fs.ko\\nKernel space\\nExt4 FS\\nNIC\\n(remote)\\n/dev/nvme1\\n\\nStorage node having front-end NVMe over Fabric block support\\nNIC\\nBlock service\\n\\nNIC\\nFile service\\nMode 2: “NFSoRDMA”\\nIB or RoCE switch\\nStorage node having front-end NFS over RDMA support\\nNFS mount\\n/dev/nvme0\\nLocal NVMe drive\\nMode 0: local NVMe drive\\n\\nCUDA 11.0\\nSource: https://developer.nvidia.com/gpudirect-storage, Key features: for local NVMe with ext4, NVMeoF with ext4, NFSoRDMA \\n\\n\\nSlide #9: \\nFUTUREWEI INTERNAL\\n10\\n\\n Image: a series of photos showing different types of electronic devices\\n\\n\\nSome vendors may choose to use special DFS drivers. Our initial approach. file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nImage: a black and white photo of a black and white tv\\n\\n\\n\\nSlide #6: \\nCommon data solution reference architecture (open-source)\\nFUTUREWEI INTERNAL\\n7\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost 1\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost n\\n\\nMulti-host connected by high speed RDMA network (IB/RoCE)\\nNIC\\nNIC\\n\\nRemote storage\\nNIC\\n\\nConnect to storage with NVMe over Fabric (IB/RoCE)\\nLUNs\\nFiles\\nObjects\\nNVMe driver\\nCUDA\\nData lake\\nAI\\nADAS\\nCommon\\nPython3\\nCUDA-X\\nDocker\\nK8S\\nJava\\nGCC\\nScala\\nSpark core\\nSystem\\nConfiguration\\nSoftware\\nConfiguration\\nTensorFlow\\nPytorch\\nSpark SQL\\nHive\\nPresto\\nAlluxio\\nTPC-DS\\nBigBench v2\\nBDD 100K\\nSpark ML\\nFlink\\nSpark Stream\\nHorovod\\nRay\\nGPU driver\\nJupyter hub\\nApollo\\nCARLA\\nErdos\\nAWS\\nAzure\\nGoogle Cloud\\nHPC\\nopenHPC\\ntest-suite-ohpc\\nMySQL\\nKeras\\nNvidia container runtime\\nGPU scheduler? Prometheus\\nGrafana\\nKafka\\nArgo workflow\\nGPU monitoring\\nVertical (ADAS, BIO) training\\nSimulation\\nNFS\\n\\nExtended solution\\n\\n\\n\\nSlide #7: \\nBenchmark selection/adoption\\nNeed to have storage needs (large data sets, IO bound, etc)\\nCandidates\\nBDD100k (ADAS related) (2TB)\\nSimulation and visualizing NASA Mars Lander data set (128TB)\\nSimulate, analyze, and visualize molecular dynamics dataset (17TB)\\nTPCx-BB express benchmark BB (Hadoop based big data system), IO intensity queries. TPC-H\\nGDS for PyTorch (GDS numpy, GDS numpy DALI, GDS numpy 1-file with DeepLabv3+), on CAM5 climate data set to predict extreme weather patterns. Prometheus\\nGrafana\\nKafka\\nArgo workflow\\nGPU monitoring\\nVertical (ADAS, BIO) training\\nSimulation\\nNFS\\n\\nExtended solution\\n\\n\\n\\nSlide #7: \\nBenchmark selection/adoption\\nNeed to have storage needs (large data sets, IO bound, etc)\\nCandidates\\nBDD100k (ADAS related) (2TB)\\nSimulation and visualizing NASA Mars Lander data set (128TB)\\nSimulate, analyze, and visualize molecular dynamics dataset (17TB)\\nTPCx-BB express benchmark BB (Hadoop based big data system), IO intensity queries. TPC-H\\nGDS for PyTorch (GDS numpy, GDS numpy DALI, GDS numpy 1-file with DeepLabv3+), on CAM5 climate data set to predict extreme weather patterns. FUTUREWEI INTERNAL\\n8\\n\\n\\nSlide #8: \\nNvidia GPUDirect Storage (GDS) support, v0.9 Nov 2020\\nFUTUREWEI INTERNAL\\n9\\nMode 1: “NVMeoF with ext4”\\nTesla GPU\\n\\nlibcufile.so\\nUser space\\n\\n\\nNvidia-fs.ko\\nKernel space\\nExt4 FS\\nNIC\\n(remote)\\n/dev/nvme1\\n\\nStorage node having front-end NVMe over Fabric block support\\nNIC\\nBlock service\\n\\nNIC\\nFile service\\nMode 2: “NFSoRDMA”\\nIB or RoCE switch\\nStorage node having front-end NFS over RDMA support\\nNFS mount\\n/dev/nvme0\\nLocal NVMe drive\\nMode 0: local NVMe drive\\n\\nCUDA 11.0\\nSource: https://developer.nvidia.com/gpudirect-storage, Key features: for local NVMe with ext4, NVMeoF with ext4, NFSoRDMA \\n\\n\\nSlide #9: \\nFUTUREWEI INTERNAL\\n10\\n\\n Image: a series of photos showing different types of electronic devices\\n\\n\\nSome vendors may choose to use special DFS drivers.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #10: \\nPart 1: Single host environment with GDS\\nSingle-host “DGX” simulation installation\\nNot including all the AI/ML methods but include a training example. Not including k8s (GPU scheduling) but include basic containers using Nvidia container runtime. Not including remote storage connection\\nGDS Mode 1 testing with simulated storage devices\\nNot using real OceanStor v6 (not available)\\nGDS Mode 2 testing with simulated storage devices\\nNot using real 100D (no real NVMeoF NAS available to us) or OceanStor v6\\nSingle-host configuration wrap up with deliverables\\nInitiate collaboration with Nvidia for DGX-2\\nFUTUREWEI INTERNAL\\n11\\n\\n\\nSlide #11: \\nPart 2: Distributed, multi-host, multi-tenancy environment and best practice\\nDistributed, Multi-GPU host configuration need more cards, more investment. Challenges include GPU scheduling\\nmulti-tenancy and GPU aware \\nGPU aware K8s\\nVM environment (whether it’s popular, vmware solution)\\nPerformance evaluation\\nDifferent scheduling methods in a cluster environment\\nDifferent GDS modes\\nDifferent AI workloads\\n\\nFUTUREWEI INTERNAL\\n12\\n\\n\\nSlide #12: \\nUse case investigation\\nPotential customers and possible use cases\\nFor example, oil companies. RFI analysis related to AI intelligent systems\\nEMEA market\\nFuture RFI answering\\nAnalysis for current and future market \\nFUTUREWEI INTERNAL\\n13\\nPart 3: Market & use case research\\n\\n\\nSlide #13: \\nDeliverable#1 Published Paper for Futurewei’s data platform solution\\nIncluding support for GPU heavy system like DGX-2\\nGPUDirect support\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nComparison between different GDS modes\\nMarket analysis for AI intelligent systems. Challenges include GPU scheduling\\nmulti-tenancy and GPU aware \\nGPU aware K8s\\nVM environment (whether it’s popular, vmware solution)\\nPerformance evaluation\\nDifferent scheduling methods in a cluster environment\\nDifferent GDS modes\\nDifferent AI workloads\\n\\nFUTUREWEI INTERNAL\\n12\\n\\n\\nSlide #12: \\nUse case investigation\\nPotential customers and possible use cases\\nFor example, oil companies. RFI analysis related to AI intelligent systems\\nEMEA market\\nFuture RFI answering\\nAnalysis for current and future market \\nFUTUREWEI INTERNAL\\n13\\nPart 3: Market & use case research\\n\\n\\nSlide #13: \\nDeliverable#1 Published Paper for Futurewei’s data platform solution\\nIncluding support for GPU heavy system like DGX-2\\nGPUDirect support\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nComparison between different GDS modes\\nMarket analysis for AI intelligent systems. FUTUREWEI INTERNAL\\n14\\n\\n\\nSlide #14: \\nDeliverable#2 Open-source config scripts\\nFUTUREWEI INTERNAL\\n15\\nAnsible\\ninventory\\n\\nhost1\\nhost2\\nVMs temporarily not supported\\nplaybook\\ninfra\\n\\n Image: a yellow and orange striped piece of paper\\n\\ncommon\\n\\n Image: a yellow and orange striped piece of paper\\n\\nai\\n\\n Image: a yellow and orange striped piece of paper\\n\\ndatalake\\n\\n Image: a yellow and orange striped piece of paper\\n\\nadas\\n\\n Image: a yellow and orange striped piece of paper\\n\\nhpc\\nPlan to create very basic ansible steps and test programs as open-source projects. Not including k8s (GPU scheduling) but include basic containers using Nvidia container runtime. Not including remote storage connection\\nGDS Mode 1 testing with simulated storage devices\\nNot using real OceanStor v6 (not available)\\nGDS Mode 2 testing with simulated storage devices\\nNot using real 100D (no real NVMeoF NAS available to us) or OceanStor v6\\nSingle-host configuration wrap up with deliverables\\nInitiate collaboration with Nvidia for DGX-2\\nFUTUREWEI INTERNAL\\n11\\n\\n\\nSlide #11: \\nPart 2: Distributed, multi-host, multi-tenancy environment and best practice\\nDistributed, Multi-GPU host configuration need more cards, more investment. Challenges include GPU scheduling\\nmulti-tenancy and GPU aware \\nGPU aware K8s\\nVM environment (whether it’s popular, vmware solution)\\nPerformance evaluation\\nDifferent scheduling methods in a cluster environment\\nDifferent GDS modes\\nDifferent AI workloads\\n\\nFUTUREWEI INTERNAL\\n12\\n\\n\\nSlide #12: \\nUse case investigation\\nPotential customers and possible use cases\\nFor example, oil companies. file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #10: \\nPart 1: Single host environment with GDS\\nSingle-host “DGX” simulation installation\\nNot including all the AI/ML methods but include a training example. Not including k8s (GPU scheduling) but include basic containers using Nvidia container runtime. Not including remote storage connection\\nGDS Mode 1 testing with simulated storage devices\\nNot using real OceanStor v6 (not available)\\nGDS Mode 2 testing with simulated storage devices\\nNot using real 100D (no real NVMeoF NAS available to us) or OceanStor v6\\nSingle-host configuration wrap up with deliverables\\nInitiate collaboration with Nvidia for DGX-2\\nFUTUREWEI INTERNAL\\n11\\n\\n\\nSlide #11: \\nPart 2: Distributed, multi-host, multi-tenancy environment and best practice\\nDistributed, Multi-GPU host configuration need more cards, more investment. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch. But in research field, researcher prefer Pytorch over TensorFlow.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is Nvidia's ecosystem\"\n",
    "\n",
    "\n",
    "for item in ns.start_chat_stream(query):\n",
    "    if isinstance(item, str):\n",
    "        # This is a token, print or process as needed\n",
    "        print(item, end='', flush=True)\n",
    "    else:\n",
    "        # This is the final response with metadata\n",
    "        print(\"\\n\\nFull response:\", item['response'])\n",
    "        print(\"Metadata:\", item['metadata'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild the index when either of the  followings are changed:\n",
    "- openai_model_yn\n",
    "- embedding_model\n",
    "- language_model\n",
    "- chroma_db_dir\n",
    "- index_persist_dir\n",
    "- chroma_collection_name\n",
    "- chunk_overlap\n",
    "- chunk_size\n",
    "- recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:10:04,517 - nexusync.core.indexing_functions - INFO - Starting index rebuild process...\n",
      "2024-10-09 17:10:04,521 - nexusync.utils.embedding_models.set_embedding_model - INFO - Using OpenAI embedding model: text-embedding-3-large\n",
      "2024-10-09 17:10:04,526 - nexusync.utils.embedding_models.set_language_model - INFO - Using OpenAI LLM model: gpt-4o-mini\n",
      "2024-10-09 17:10:04,527 - nexusync.core.indexing_functions - INFO - Deleting existing index directory: index_storage\n",
      "2024-10-09 17:10:04,535 - nexusync.core.indexing_functions - INFO - Deleting existing Chroma DB directory: chroma_db\n",
      "2024-10-09 17:10:04,538 - nexusync.core.indexing_functions - WARNING - Index not found. Building a new index.\n",
      "2024-10-09 17:10:13,819 - nexusync.core.indexing_functions - INFO - Loaded 40 chunks from ../sample_docs.\n",
      "2024-10-09 17:10:16,163 - nexusync.core.indexing_functions - INFO - Index Built.\n"
     ]
    }
   ],
   "source": [
    "from nexusync import rebuild_index\n",
    "\n",
    "OPENAI_MODEL_YN = True # if False, you will use ollama model\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\" # suggested embedding model\n",
    "LANGUAGE_MODEL = 'gpt-4o-mini' # you need to download ollama model first, please check https://ollama.com/download\n",
    "TEMPERATURE = 0.4 # range from 0 to 1, higher means higher creativitiy level\n",
    "CHROMA_DB_DIR = 'chroma_db'\n",
    "INDEX_PERSIST_DIR = 'index_storage'\n",
    "CHROMA_COLLECTION_NAME = 'my_collection'\n",
    "INPUT_DIRS = [\"../sample_docs\"] # can specify multiple document paths\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 20\n",
    "RECURSIVE = True\n",
    "\n",
    "rebuild_index(input_dirs=INPUT_DIRS, \n",
    "              openai_model_yn=OPENAI_MODEL_YN, \n",
    "              embedding_model=EMBEDDING_MODEL, \n",
    "              language_model=LANGUAGE_MODEL, \n",
    "              temperature=TEMPERATURE, \n",
    "              chroma_db_dir = CHROMA_DB_DIR,\n",
    "              index_persist_dir = INDEX_PERSIST_DIR,\n",
    "              chroma_collection_name=CHROMA_COLLECTION_NAME,\n",
    "              chunk_overlap=CHUNK_OVERLAP,\n",
    "              chunk_size=CHUNK_SIZE,\n",
    "              recursive=RECURSIVE\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:10:16,169 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-09 17:10:16,171 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-09 17:10:25,050 - nexusync.core.indexer - INFO - Loaded 6 files from ../sample_docs\n",
      "2024-10-09 17:10:25,060 - nexusync.core.indexer - INFO - Updated 1 files in ../sample_docs\n",
      "2024-10-09 17:10:25,060 - nexusync.core.indexer - INFO - Updated file: /mnt/d/nexusync/notebooks/../sample_docs/new_added.txt\n",
      "2024-10-09 17:10:25,062 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,062 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,062 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,062 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,063 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,063 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,063 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,064 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,064 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\n",
      "2024-10-09 17:10:25,065 - nexusync.core.indexer - INFO - Deleting 9 files from the index.\n",
      "2024-10-09 17:10:25,067 - nexusync.core.indexer - INFO - Deletion process completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and persist the vector store in a chroma db\n",
    "ns.refresh_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-time query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 06:59:57,194 - nexusync.NexuSync - INFO - Starting query: how to install nexusync\n",
      "2024-10-09 07:00:01,795 - nexusync.NexuSync - INFO - Query completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how to install nexusync\n",
      "Response: To install Nexusync, use the following command:\n",
      "\n",
      "pip install nexusync\n",
      "Response: {'sources': [{'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\\n\\nInstallation\\n\\n```bash\\npip install nexusync\\n```', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/READEME.md', 'file_name': 'READEME.md', 'file_type': 'text/markdown', 'file_size': 4571, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\\n\\nNexuSync\\n\\nNexuSync is a powerful document indexing and querying tool built on top of LlamaIndex. It allows you to efficiently manage, search, and interact with large collections of documents using advanced natural language processing techniques.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/READEME.md', 'file_name': 'READEME.md', 'file_type': 'text/markdown', 'file_size': 4571, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\\n\\n🔧 Why NexuSync is Handy\\n\\n1. **Effortless Document Management**: \\n   NexuSync automatically handles document indexing, updating, and deletion. 5. **Scalability**: \\n   NexuSync efficiently handles large document collections, making it suitable for personal knowledge bases to enterprise-scale document management. file_path: /mnt/d/nexusync/notebooks/../sample_docs/READEME.md\\n\\n🔧 Why NexuSync is Handy\\n\\n1. **Effortless Document Management**: \\n   NexuSync automatically handles document indexing, updating, and deletion. You don\\'t need to manually manage your document database. You don\\'t need to manually manage your document database. ```python\\n   # After adding new documents to your directory\\n   ns.refresh_index()\\n   # NexuSync automatically detects and indexes new or modified documents\\n   ```\\n\\n2. **Natural Language Queries**: \\n   Instead of complex search syntax, use natural language to find information. ```python\\n   ns.chat(\"Explain the concept of machine learning.\") ns.chat(\"How does it differ from deep learning?\") # The chat maintains context, providing coherent follow-up responses\\n   ```\\n\\n4. **Natural Language Queries**: \\n   Instead of complex search syntax, use natural language to find information. ```python\\n   response = ns.query(\"What are the main features of Python?\") print(response[\\'response\\'])\\n   # Get a concise summary of Python\\'s main features from your documents\\n   ```\\n\\n3. **Conversational Interface**: \\n   Interact with your documents as if you\\'re chatting with an AI assistant. ```python\\n   ns.chat(\"Explain the concept of machine learning.\") ns.chat(\"How does it differ from deep learning?\")', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/READEME.md', 'file_name': 'READEME.md', 'file_type': 'text/markdown', 'file_size': 4571, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"how to install nexusync\"\n",
    "\n",
    "\n",
    "response = ns.start_query(text_qa_template = text_qa_template, query = query )\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "print(f\"Response: {response['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat History:\n",
      "Human: summary of the store.json?\n",
      "AI: The `store.json` file contains a collection of products, each represented by an object with the following attributes:\n",
      "\n",
      "1. **id**: A unique identifier for each product.\n",
      "2. **title**: The name of the product.\n",
      "3. **price**: The cost of the product.\n",
      "4. **description**: A brief description of the product.\n",
      "5. **category**: The category to which the product belongs (e.g., men's clothing, women's clothing, jewelry, electronics).\n",
      "6. **image**: A URL link to an image of the product.\n",
      "7. **rating**: An object containing:\n",
      "   - **rate**: The average rating of the product.\n",
      "   - **count**: The number of ratings received.\n",
      "\n",
      "The data includes a variety of products spanning different categories such as:\n",
      "- **Men's Clothing** (e.g., backpacks, jackets, t-shirts)\n",
      "- **Women's Clothing** (e.g., casual t-shirts, short sleeves)\n",
      "- **Jewelry** (e.g., bracelets, rings)\n",
      "- **Electronics** (e.g., external hard drives, SSDs)\n",
      "\n",
      "The file contains detailed descriptions and ratings for each product, providing insight into their features and customer feedback.\n",
      "\n",
      "Human: price for Mens Casual Premium Slim Fit T-Shirts?\n",
      "AI: The price for the \"Mens Casual Premium Slim Fit T-Shirts\" is $22.99.\n",
      "\n",
      "Human: recheck it?\n",
      "AI: I apologize for the confusion, but I currently do not have access to the contents of the `store.json` file to recheck the price for the \"Mens Casual Premium Slim Fit T-Shirts.\" If you have access to the file, you can open it and search for the product to verify its price. If you need help with a specific method to check it, please let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get chat history\n",
    "chat_history = ns.chat_engine.get_chat_history()\n",
    "print(\"Chat History:\")\n",
    "for entry in chat_history:\n",
    "    print(f\"Human: {entry['query']}\")\n",
    "    print(f\"AI: {entry['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Refresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 10:35:59,214 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-09 10:35:59,216 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-09 10:36:08,515 - nexusync.core.indexer - INFO - Loaded 6 files from ../sample_docs\n",
      "2024-10-09 10:36:08,536 - nexusync.core.indexer - INFO - Updated 1 files in ../sample_docs\n",
      "2024-10-09 10:36:08,537 - nexusync.core.indexer - INFO - Updated file: /mnt/d/nexusync/notebooks/../sample_docs/new_added.txt\n",
      "2024-10-09 10:36:08,538 - nexusync.core.indexer - INFO - No deleted files found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add a new document\n",
    "with open(\"../sample_docs/new_added.txt\", \"w\") as f:\n",
    "    f.write(\"Breaking News: Trump and Harris had a fight!!!!\")\n",
    "\n",
    "# Refresh the index: incremental in new files and detect deleted files in the folder\n",
    "ns.refresh_index()\n",
    "print(\"Index refreshed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:40:27,349 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-07 15:40:27,353 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-07 15:40:36,886 - nexusync.core.indexer - INFO - Loaded 7 files from ../sample_docs\n",
      "2024-10-07 15:40:36,887 - nexusync.core.indexer - INFO - Updated 0 files in ../sample_docs\n",
      "2024-10-07 15:40:36,888 - nexusync.core.indexer - INFO - No deleted files found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed after deletion.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Delete the new document\n",
    "# os.remove('../sample_docs/Nvidia ecosystem.pptx')\n",
    "# print(\"New document deleted.\")\n",
    "\n",
    "ns.refresh_index()\n",
    "print(\"Index refreshed after deletion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexusync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
