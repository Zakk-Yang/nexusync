{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NexuSync Demo Notebook\n",
    "\n",
    "This notebook demonstrates the basic usage of NexuSync for document indexing and querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NexuSync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zy-wsl/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace embedding model: BAAI/bge-base-en-v1.5\n",
      "Using Ollama LLM model: llama3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:51:50,816 - nexusync.core.indexer - INFO - Index already built. Loading from disk.\n"
     ]
    }
   ],
   "source": [
    "from nexusync.models import set_embedding_model, set_language_model\n",
    "from nexusync import NexuSync\n",
    "import os\n",
    "\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "LLM_MODEL = 'llama3.2'\n",
    "TEMPERATURE = 0.4\n",
    "INPUT_DIRS = [\"../sample_docs\"] # can put multiple paths\n",
    "\n",
    "set_embedding_model(huggingface_model= EMBEDDING_MODEL) \n",
    "set_language_model(ollama_model = LLM_MODEL, temperature=TEMPERATURE)\n",
    "ns = NexuSync(input_dirs=INPUT_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information above, I want you to think step by step to answer the query in a crisp manner. \"\n",
    "    \"In case you don't know the answer, say 'I don't know!'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-time query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: News about Nvidia?\n",
      "Response: Based on the provided context information, here are the key points related to news about Nvidia:\n",
      "\n",
      "1. Nvidia's CEO Jensen Huang made a bombshell announcement that raised the bar for the stock.\n",
      "2. Nvidia plans to ship Blackwell GPUs to clients in Q4 of this year, with a consumer release expected in 2025.\n",
      "3. The demand for Blackwell is \"insane,\" and Nvidia forecasts $32.5 billion in revenue for the current quarter, an 80% increase from last year.\n",
      "4. Nvidia's stock has surged by over 150% this year, following an impressive 240% gain in 2023.\n",
      "5. Major cloud providers like AWS, Azure, and Google Cloud are integrating Blackwell into their infrastructure to support high-performance AI workloads.\n",
      "6. Oracle announced that it would need 131,072 Nvidia Blackwell GPUs as part of a $6.5 billion investment to establish a new public cloud region in Malaysia.\n",
      "\n",
      "These points suggest that Nvidia is making significant advancements in its artificial intelligence (AI) technology and is expected to see substantial growth in revenue and stock performance.\n",
      "Response: {'sources': [{'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nNvidia CEO\\'s bombshell raises the bar for the stock\\n\\nSilin Chen\\n\\nSat, October 5, 2024 at 2:03 PM GMT+1\\xa03 min read\\n\\n35\\n\\nIn This Article:\\n\\nStockStory Top Pick\\n\\n\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\nThink of how Nvidia’s GeForce 8800 chip, launched in 2006, changed the gaming landscape. Now, almost two decades later, Nvidia is still making that progression, with its Blackwell designed to change the world of artificial intelligence. Nvidia forecasts $32.5 billion in revenue for the current quarter, an 80% increase from last year. Related: Veteran trader targets Nvidia as shares slide\\n\\nNvidia plans to ship Blackwell GPUs to clients in Q4 of this year, with a consumer release expected in 2025. “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia Chief Financial Officer Colette Kress said during the August earnings call. “The demand for Blackwell is insane. Everybody wants to have the most, and everybody wants to be first.”\\n\\n\\n\\nNvidia\\'s stock has surged by over 150% this year, following an impressive 240% gain in 2023. Hyperscaler buyers like Amazon\\xa0\\xa0(AMZN)\\xa0, Microsoft\\xa0\\xa0(MSFT)\\xa0, and Alphabet\\xa0\\xa0(GOOGL)\\xa0\\xa0are expected to spend around $160 billion in 2024 on AI infrastructure, according to Bernstein analysts. “If we can increase the performance, like we\\'ve done for Hopper and Blackwell ... we\\'re effectively increasing the revenue or throughput for our customers on these infrastructures by a couple to three times each year,\" Huang added. Nvidia\\'s financial performance exceeds expectations\\n\\nNvidia’s latest earnings report further solidifies its strong position in the AI market. On August 28, the company posted earnings per share of 68 cents, beating Wall Street expectations of 64 cents. Story Continues\\n\\nView Comments\\xa0(35)\\n\\nTerms\\n\\n\\xa0and\\xa0\\n\\nPrivacy Policy\\n\\nPrivacy & Cookie Settings\\n\\n\\n\\n\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC Major cloud providers like AWS, Azure, and Google Cloud are integrating Blackwell into their infrastructure to support high-performance AI workloads. Related: Nvidia CEO Jensen Huang just told investors what’s next for the AI chipmaker\\n\\nOracle\\xa0announced\\xa0on October 2 that it would need 131,072 Nvidia Blackwell GPUs as part of a $6.5 billion investment to establish a new public cloud region in Malaysia, another proof of a strong need for advanced AI processing capabilities. Blackwell is a platform Nvidia launched in March that allows organizations to run real-time generative AI on models with trillions of parameters. Now, almost two decades later, Nvidia is still making that progression, with its Blackwell designed to change the world of artificial intelligence. Demand is \"insane,\" Nvidia’s chief executive Jensen Huang recently said. Major cloud providers like AWS, Azure, and Google Cloud are integrating Blackwell into their infrastructure to support high-performance AI workloads. Revenue hit $30.04 billion, up 122%, surpassing the anticipated $28.7 billion. Nvidia forecasts $32.5 billion in revenue for the current quarter, an 80% increase from last year. Related: Veteran trader targets Nvidia as shares slide\\n\\nNvidia plans to ship Blackwell GPUs to clients in Q4 of this year, with a consumer release expected in 2025. Related: Nvidia CEO Jensen Huang just told investors what’s next for the AI chipmaker\\n\\nOracle\\xa0announced\\xa0on October 2 that it would need 131,072 Nvidia Blackwell GPUs as part of a $6.5 billion investment to establish a new public cloud region in Malaysia, another proof of a strong need for advanced AI processing capabilities. Blackwell is a platform Nvidia launched in March that allows organizations to run real-time generative AI on models with trillions of parameters. These large language models are trained on extensive datasets to understand and generate responses in human language. The cost of Blackwell is expected to range between $30,000 and $40,000 per unit. Huang emphasized the importance of continuous updates to Nvidia’s AI infrastructure, with the company releasing new platforms annually. “If we can increase the performance, like we\\'ve done for Hopper and Blackwell ... we\\'re effectively increasing the revenue or throughput for our customers on these infrastructures by a couple to three times each year,\" Huang added. Related: Veteran trader targets Nvidia as shares slide\\n\\nNvidia plans to ship Blackwell GPUs to clients in Q4 of this year, with a consumer release expected in 2025. “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia Chief Financial Officer Colette Kress said during the August earnings call. Story Continues\\n\\nView Comments\\xa0(35)\\n\\nTerms\\n\\n\\xa0and\\xa0\\n\\nPrivacy Policy\\n\\nPrivacy & Cookie Settings\\n\\n\\n\\n\\n\\nPalantir Stock vs.', 'metadata': {'file_name': 'news.docx', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/news.docx', 'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'file_size': 274441, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #14: \\nDeliverable#2 Open-source config scripts\\nFUTUREWEI INTERNAL\\n15\\nAnsible\\ninventory\\n\\nhost1\\nhost2\\nVMs temporarily not supported\\nplaybook\\ninfra\\n\\n Image: a yellow and orange striped piece of paper\\n\\ncommon\\n\\n Image: a yellow and orange striped piece of paper\\n\\nai\\n\\n Image: a yellow and orange striped piece of paper\\n\\ndatalake\\n\\n Image: a yellow and orange striped piece of paper\\n\\nadas\\n\\n Image: a yellow and orange striped piece of paper\\n\\nhpc\\nPlan to create very basic ansible steps and test programs as open-source projects. Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': \"file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC\\n\\n\\n\\nPalantir\\xa0Technologies\\xa0(NYSE: PLTR)\\xa0and\\xa0Nvidia\\xa0(NASDAQ: NVDA)\\xa0are two of the hottest\\xa0artificial intelligence (AI) stocks\\xa0on Wall Street. In fact, with year-to-date returns of 132% and 150%, respectively, they rank among the five best-performing components of the\\xa0S&P 500. Among the 23 analysts who follow Palantir, the median price target is $27 per share, which implies 32% downside from its current share price of $40. Among the 65 analysts following Nvidia, the median price target is $150 per share, which implies 20% upside from the current share price of $125. Furthermore, Palantir is the most overvalued stock in the S&P 500 based on the difference between its current price and median price target. Meanwhile, according to\\xa0FactSet Research, Nvidia ranks among the most highly recommended stocks in the S&P 500 in terms of its percentage of buy ratings. Suffice it to say Wall Street is overwhelmingly bearish on Palantir but very bullish on Nvidia. Here are the most important details for investors. Furthermore, Palantir is the most overvalued stock in the S&P 500 based on the difference between its current price and median price target. Meanwhile, according to\\xa0FactSet Research, Nvidia ranks among the most highly recommended stocks in the S&P 500 in terms of its percentage of buy ratings. Suffice it to say Wall Street is overwhelmingly bearish on Palantir but very bullish on Nvidia. file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC\\n\\n\\n\\nPalantir\\xa0Technologies\\xa0(NYSE: PLTR)\\xa0and\\xa0Nvidia\\xa0(NASDAQ: NVDA)\\xa0are two of the hottest\\xa0artificial intelligence (AI) stocks\\xa0on Wall Street. That means some clients find Palantir's software so complex that they struggle to use it independently. Gartner also omitted Palantir in its latest report on data science and machine learning platforms. However, Wall Street expects the stocks to move in opposite directions over the next year. Among the 23 analysts who follow Palantir, the median price target is $27 per share, which implies 32% downside from its current share price of $40. Among the 65 analysts following Nvidia, the median price target is $150 per share, which implies 20% upside from the current share price of $125. Suffice it to say Wall Street is overwhelmingly bearish on Palantir but very bullish on Nvidia. Here are the most important details for investors. Palantir Technologies: 32% downside implied by the median price target\\n\\nPalantir has deep roots in counterterrorism and clandestine military operations. Some analysts have lauded Palantir for its sophisticated technology. For instance, it was a top-ranked vendor in Dresner Advisory Services' 2024 market study on artificial intelligence, data science, and machine learning platforms. Forrester Research\\xa0recently recognized its leadership in AI and machine learning platforms. But it has since expanded its customer base to include international governments and commercial organizations. Palantir's data operations platforms, Foundry and Gotham, let customers incorporate data and machine learning models into analytical applications that improve decision-making. And its AI platform, AIP, allows commercial and government clients to use\\xa0large language models\\xa0and generative AI within Foundry and Gotham. And its AI platform, AIP, allows commercial and government clients to use\\xa0large language models\\xa0and generative AI within Foundry and Gotham. Some analysts have lauded Palantir for its sophisticated technology. For instance, it was a top-ranked vendor in Dresner Advisory Services' 2024 market study on artificial intelligence, data science, and machine learning platforms.\", 'metadata': {'file_name': 'news.docx', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/news.docx', 'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'file_size': 274441, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"News about Nvidia?\"\n",
    "\n",
    "\n",
    "response = ns.query(text_qa_template = text_qa_template, query = query )\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "print(f\"Response: {response['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:52:43,787 - nexusync.core.chat_engine - INFO - Chat engine initialized\n"
     ]
    }
   ],
   "source": [
    "# Initiate the chat engine once\n",
    "ns.chat_engine.initialize_chat_engine(text_qa_template, chat_mode=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: how many GPUs will Nvidia get from Oracle?\n",
      "AI: The text does not mention that Oracle plans to order any GPUs from Nvidia. It actually mentions that Futurewei (not Oracle) would need 131,072 Nvidia Blackwell GPUs as part of a $6.5 billion investment to establish a new public cloud region in Malaysia.\n",
      "\n",
      "Additionally, the text also mentions Palantir Technologies and its relationship with Nvidia, but it does not mention any GPU orders from Nvidia by either company, including Oracle.\n",
      "\n",
      "METADATA: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\n",
      "Human: what is its ecosystem?\n",
      "AI: Based on the provided context information, it appears that Nvidia's ecosystem refers to the company's comprehensive platform and tools for developing, deploying, and managing AI, data science, and high-performance computing (HPC) applications.\n",
      "\n",
      "The text mentions various components of Nvidia's ecosystem, including:\n",
      "\n",
      "1. GPUs: Nvidia graphics processing units are used for accelerating AI, data science, and HPC workloads.\n",
      "2. CUDA: A parallel computing platform and programming model developed by Nvidia.\n",
      "3. Deep learning frameworks: Nvidia supports popular deep learning frameworks such as TensorFlow, PyTorch, and Keras.\n",
      "4. Nvidia Drive PX: A platform for developing autonomous vehicles.\n",
      "5. Nvidia GPU Direct (GDS): A technology that allows for high-speed storage and networking between GPUs.\n",
      "\n",
      "Nvidia's ecosystem also includes various tools, frameworks, and platforms for developing AI, data science, and HPC applications, such as:\n",
      "\n",
      "1. NGC registry: A container registry that provides a platform for deploying and managing containers on Nvidia GPUs.\n",
      "2. Container runtime: Nvidia provides a container runtime that allows developers to run containers on Nvidia GPUs.\n",
      "\n",
      "Additionally, the text mentions various software ecosystems from Nvidia, including:\n",
      "\n",
      "1. AI software ecosystem\n",
      "2. Big data platform solution\n",
      "\n",
      "Overall, Nvidia's ecosystem is designed to provide a comprehensive platform for developers to build, deploy, and manage AI, data science, and HPC applications on Nvidia GPUs.\n",
      "\n",
      "METADATA: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\n"
     ]
    }
   ],
   "source": [
    "# Start chatting, chat with memories\n",
    "queries = [\n",
    "    \"how many GPUs will Nvidia get from Oracle?\",\n",
    "    \"what is its ecosystem?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Human: {query}\")\n",
    "    response = ns.chat_engine.chat(query)\n",
    "    print(f\"AI: {response['response']}\\n\")\n",
    "    print(f\"METADATA: {response['metadata']['sources'][0]['metadata']['file_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat History:\n",
      "Human: What is NexuSync?\n",
      "AI: NexuSync is a powerful document indexing and querying tool built on top of LlamaIndex. It allows you to efficiently manage, search, and interact with large collections of documents using advanced natural language processing techniques.\n",
      "\n",
      "Human: What are its main features?\n",
      "AI: According to the README.md file, NexuSync has the following main features:\n",
      "\n",
      "1. **Smart Document Indexing**: Automatically index documents from specified directories, keeping your knowledge base up-to-date.\n",
      "2. **Efficient Querying**: Use natural language to query your document collection and get relevant answers quickly.\n",
      "3. **Upsert Capability**: Easily update or insert new documents into the index without rebuilding from scratch.\n",
      "4. **Deletion Handling**: Automatically remove documents from the index when they're deleted from the filesystem.\n",
      "5. **Chat Interface**: Engage in conversational interactions with your document collection, making information retrieval more intuitive.\n",
      "6. **Flexible Embedding Options**: Choose between OpenAI and HuggingFace embedding models to suit your needs and constraints.\n",
      "\n",
      "Additionally, NexuSync provides a conversational interface that allows you to interact with your documents as if you're chatting with an AI assistant.\n",
      "\n",
      "Human: How does it handle document indexing?\n",
      "AI: According to the README.md file, NexuSync automatically indexes documents from specified directories, keeping your knowledge base up-to-date. This means that:\n",
      "\n",
      "* You don't need to manually manage your document database.\n",
      "* NexuSync can detect and index new or modified documents in the directory without requiring explicit updates.\n",
      "\n",
      "To achieve this, you simply need to add new documents to the directory, and then call the `ns.refresh_index()` function to update the index. This allows you to focus on adding content rather than worrying about the underlying indexing infrastructure.\n",
      "\n",
      "Human: do you remember what we were taling about?\n",
      "AI: We were discussing NexuSync, a document indexing and querying tool. We went over its main features, including smart document indexing, efficient querying, upsert capability, deletion handling, and chat interface options.\n",
      "\n",
      "Before that, we were examining some sample documents related to the \"hdiifye2023.xlsx\" file, which appeared to be an Excel spreadsheet containing data with dates and values.\n",
      "\n",
      "We also did a comparison of two different embedding models (OpenAI and HuggingFace) for natural language processing tasks.\n",
      "\n",
      "Human: do you remember what we were taling about?\n",
      "AI: It seems that I made a mistake. We hadn't started discussing anything yet. You provided some file paths and a few lines of text, but there was no context or previous conversation to recall. Let's start fresh! What would you like to talk about?\n",
      "\n",
      "Human: do you remember what we were taling about?\n",
      "AI: No, I don't have any memory of our previous conversation. You provided some file paths and a few lines of text, but there was no context or previous discussion to recall. Each time you interact with me, it's a new conversation. Would you like to start fresh and discuss something new?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get chat history\n",
    "chat_history = ns.chat_engine.get_chat_history()\n",
    "print(\"Chat History:\")\n",
    "for entry in chat_history:\n",
    "    print(f\"Human: {entry['query']}\")\n",
    "    print(f\"AI: {entry['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Chat (word by word output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:54:17,669 - nexusync.core.chat_engine - INFO - Chat engine initialized\n"
     ]
    }
   ],
   "source": [
    "# Initiate the chat engine once\n",
    "ns.chat_engine.initialize_chat_engine(text_qa_template, chat_mode=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nvidia ecosystem refers to the collection of technologies, products, and services developed by NVIDIA Corporation that support its graphics processing units (GPUs) and other computing hardware. The ecosystem includes:\n",
      "\n",
      "1. GPUs: NVIDIA's graphics processing units, which are used for a wide range of applications, including gaming, professional visualization, artificial intelligence, and high-performance computing.\n",
      "2. CUDA: A parallel computing platform and programming model developed by NVIDIA, which allows developers to harness the power of GPUs for general-purpose computing.\n",
      "3. Deep Learning: NVIDIA provides a range of tools and technologies for deep learning, including Tensor Cores, cuDNN, and Deep Learning SDKs.\n",
      "4. Nvidia Drive: A platform for developing autonomous vehicles, which includes GPUs, software development kits, and other technologies.\n",
      "5. Nvidia Grid: A cloud-based platform for delivering high-performance computing resources to businesses and researchers.\n",
      "6. Nvidia DGX: A line of pre-configured systems that provide a complete solution for AI, deep learning, and high-performance computing.\n",
      "7. Nvidia Quadro: A range of professional-grade GPUs designed for workstations and other applications requiring high performance and reliability.\n",
      "8. Nvidia Tesla: A line of datacenter-focused GPUs designed for high-performance computing, artificial intelligence, and other applications.\n",
      "\n",
      "The Nvidia ecosystem also includes a range of software tools and technologies, such as:\n",
      "\n",
      "1. Nvidia GPU Driver: The proprietary driver software that manages the interaction between NVIDIA GPUs and operating systems.\n",
      "2. Nvidia CUDA Toolkit: A set of development tools and libraries for building applications on top of CUDA.\n",
      "3. Nvidia Deep Learning SDKs: A range of software development kits for deep learning applications.\n",
      "4. Nvidia Tensor Cores: A technology that enables fast matrix multiplication and other linear algebra operations on GPUs.\n",
      "\n",
      "The Nvidia ecosystem is designed to provide a comprehensive platform for developers, researchers, and businesses to build and deploy high-performance computing applications, including those related to artificial intelligence, machine learning, and data science.\n",
      "\n",
      "In the context of the provided text, it appears that the Nvidia ecosystem is being explored as part of a research project or collaboration between Futurewei and NVIDIA. The text mentions various components of the ecosystem, such as GPUs, CUDA, and Deep Learning SDKs, and discusses potential applications and use cases for these technologies.{'response': \"The Nvidia ecosystem refers to the collection of technologies, products, and services developed by NVIDIA Corporation that support its graphics processing units (GPUs) and other computing hardware. The ecosystem includes:\\n\\n1. GPUs: NVIDIA's graphics processing units, which are used for a wide range of applications, including gaming, professional visualization, artificial intelligence, and high-performance computing.\\n2. CUDA: A parallel computing platform and programming model developed by NVIDIA, which allows developers to harness the power of GPUs for general-purpose computing.\\n3. Deep Learning: NVIDIA provides a range of tools and technologies for deep learning, including Tensor Cores, cuDNN, and Deep Learning SDKs.\\n4. Nvidia Drive: A platform for developing autonomous vehicles, which includes GPUs, software development kits, and other technologies.\\n5. Nvidia Grid: A cloud-based platform for delivering high-performance computing resources to businesses and researchers.\\n6. Nvidia DGX: A line of pre-configured systems that provide a complete solution for AI, deep learning, and high-performance computing.\\n7. Nvidia Quadro: A range of professional-grade GPUs designed for workstations and other applications requiring high performance and reliability.\\n8. Nvidia Tesla: A line of datacenter-focused GPUs designed for high-performance computing, artificial intelligence, and other applications.\\n\\nThe Nvidia ecosystem also includes a range of software tools and technologies, such as:\\n\\n1. Nvidia GPU Driver: The proprietary driver software that manages the interaction between NVIDIA GPUs and operating systems.\\n2. Nvidia CUDA Toolkit: A set of development tools and libraries for building applications on top of CUDA.\\n3. Nvidia Deep Learning SDKs: A range of software development kits for deep learning applications.\\n4. Nvidia Tensor Cores: A technology that enables fast matrix multiplication and other linear algebra operations on GPUs.\\n\\nThe Nvidia ecosystem is designed to provide a comprehensive platform for developers, researchers, and businesses to build and deploy high-performance computing applications, including those related to artificial intelligence, machine learning, and data science.\\n\\nIn the context of the provided text, it appears that the Nvidia ecosystem is being explored as part of a research project or collaboration between Futurewei and NVIDIA. The text mentions various components of the ecosystem, such as GPUs, CUDA, and Deep Learning SDKs, and discusses potential applications and use cases for these technologies.\", 'metadata': {'sources': [{'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #0: \\nNvidia ecosystem\\x0band how Futurewei supports it\\n\\n\\n\\nSlide #1: \\nOptimization stack\\n\\n Image: a series of photos showing different types of green plants\\n\\n\\nMulti-core, multi-thread programming library that supports vector optimization. ML, DL libraries and tools based on CUDA. Bluefield DPU, MPI tag matching, Mellanox SHARP\\nTelemetry and troubleshooting across compute, network, and storage layers. Cumulus NetQ, Mellanox UFM\\nSource : https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/\\n\\n\\nSlide #3: \\nSoftware ecosystems from Nvidia\\nNGC registry\\nhttp://ngc.nvidia.com\\n\\nContainers, models, pipelines for Nvidia GPUs. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. ML, DL libraries and tools based on CUDA. APP/data engine/AIML pipeline\\nSource: https://www.nvidia.com/en-us/data-center/magnum-io/\\n\\n\\nSlide #2: \\nFUTUREWEI INTERNAL\\n3\\n\\n Image: a collage of photos showing different types of electronic devices\\n\\n\\n Image: a green and white sign on a blue wall\\n\\nThe GPU bypasses the CPU and system memory, and accesses remote storage via 8X 200 Gb/s NICs, achieving up to 1.6Terabits/s of raw storage bandwidth. GPUDirect, Mellanox NVMe SNAP\\nNVIDIA NVLink® fabric and RDMA-based network IO acceleration reduces IO overhead, bypassing the CPU and enabling direct GPU to GPU data transfers at line rates\\nDPDK, GPUDirect RDMA, HPC-X, NCCL, NVSHEM, UCX, ASAP\\nOffloading to “network processors”. APP/data engine/AIML pipeline\\nSource: https://www.nvidia.com/en-us/data-center/magnum-io/\\n\\n\\nSlide #2: \\nFUTUREWEI INTERNAL\\n3\\n\\n Image: a collage of photos showing different types of electronic devices\\n\\n\\n Image: a green and white sign on a blue wall\\n\\nThe GPU bypasses the CPU and system memory, and accesses remote storage via 8X 200 Gb/s NICs, achieving up to 1.6Terabits/s of raw storage bandwidth. GPUDirect, Mellanox NVMe SNAP\\nNVIDIA NVLink® fabric and RDMA-based network IO acceleration reduces IO overhead, bypassing the CPU and enabling direct GPU to GPU data transfers at line rates\\nDPDK, GPUDirect RDMA, HPC-X, NCCL, NVSHEM, UCX, ASAP\\nOffloading to “network processors”. Bluefield DPU, MPI tag matching, Mellanox SHARP\\nTelemetry and troubleshooting across compute, network, and storage layers. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. Slide #5: \\nTarget #2: GPU-heavy cluster (DGX-2 like) storage reference architecture (less dependent, less expensive)\\nFUTUREWEI INTERNAL\\n6\\n…\\nServers loaded with Tesla GPUs\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv Cumulus NetQ, Mellanox UFM\\nSource : https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/\\n\\n\\nSlide #3: \\nSoftware ecosystems from Nvidia\\nNGC registry\\nhttp://ngc.nvidia.com\\n\\nContainers, models, pipelines for Nvidia GPUs. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. Slide #5: \\nTarget #2: GPU-heavy cluster (DGX-2 like) storage reference architecture (less dependent, less expensive)\\nFUTUREWEI INTERNAL\\n6\\n…\\nServers loaded with Tesla GPUs\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #14: \\nDeliverable#2 Open-source config scripts\\nFUTUREWEI INTERNAL\\n15\\nAnsible\\ninventory\\n\\nhost1\\nhost2\\nVMs temporarily not supported\\nplaybook\\ninfra\\n\\n Image: a yellow and orange striped piece of paper\\n\\ncommon\\n\\n Image: a yellow and orange striped piece of paper\\n\\nai\\n\\n Image: a yellow and orange striped piece of paper\\n\\ndatalake\\n\\n Image: a yellow and orange striped piece of paper\\n\\nadas\\n\\n Image: a yellow and orange striped piece of paper\\n\\nhpc\\nPlan to create very basic ansible steps and test programs as open-source projects. Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch. But in research field, researcher prefer Pytorch over TensorFlow.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nImage: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n\\nSlide #6: \\nCommon data solution reference architecture (open-source)\\nFUTUREWEI INTERNAL\\n7\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost 1\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost n\\n\\nMulti-host connected by high speed RDMA network (IB/RoCE)\\nNIC\\nNIC\\n\\nRemote storage\\nNIC\\n\\nConnect to storage with NVMe over Fabric (IB/RoCE)\\nLUNs\\nFiles\\nObjects\\nNVMe driver\\nCUDA\\nData lake\\nAI\\nADAS\\nCommon\\nPython3\\nCUDA-X\\nDocker\\nK8S\\nJava\\nGCC\\nScala\\nSpark core\\nSystem\\nConfiguration\\nSoftware\\nConfiguration\\nTensorFlow\\nPytorch\\nSpark SQL\\nHive\\nPresto\\nAlluxio\\nTPC-DS\\nBigBench v2\\nBDD 100K\\nSpark ML\\nFlink\\nSpark Stream\\nHorovod\\nRay\\nGPU driver\\nJupyter hub\\nApollo\\nCARLA\\nErdos\\nAWS\\nAzure\\nGoogle Cloud\\nHPC\\nopenHPC\\ntest-suite-ohpc\\nMySQL\\nKeras\\nNvidia container runtime\\nGPU scheduler? file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nImage: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n\\nSlide #6: \\nCommon data solution reference architecture (open-source)\\nFUTUREWEI INTERNAL\\n7\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost 1\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost n\\n\\nMulti-host connected by high speed RDMA network (IB/RoCE)\\nNIC\\nNIC\\n\\nRemote storage\\nNIC\\n\\nConnect to storage with NVMe over Fabric (IB/RoCE)\\nLUNs\\nFiles\\nObjects\\nNVMe driver\\nCUDA\\nData lake\\nAI\\nADAS\\nCommon\\nPython3\\nCUDA-X\\nDocker\\nK8S\\nJava\\nGCC\\nScala\\nSpark core\\nSystem\\nConfiguration\\nSoftware\\nConfiguration\\nTensorFlow\\nPytorch\\nSpark SQL\\nHive\\nPresto\\nAlluxio\\nTPC-DS\\nBigBench v2\\nBDD 100K\\nSpark ML\\nFlink\\nSpark Stream\\nHorovod\\nRay\\nGPU driver\\nJupyter hub\\nApollo\\nCARLA\\nErdos\\nAWS\\nAzure\\nGoogle Cloud\\nHPC\\nopenHPC\\ntest-suite-ohpc\\nMySQL\\nKeras\\nNvidia container runtime\\nGPU scheduler? Prometheus\\nGrafana\\nKafka\\nArgo workflow\\nGPU monitoring\\nVertical (ADAS, BIO) training\\nSimulation\\nNFS\\n\\nExtended solution\\n\\n\\n\\nSlide #7: \\nBenchmark selection/adoption\\nNeed to have storage needs (large data sets, IO bound, etc)\\nCandidates\\nBDD100k (ADAS related) (2TB)\\nSimulation and visualizing NASA Mars Lander data set (128TB)\\nSimulate, analyze, and visualize molecular dynamics dataset (17TB)\\nTPCx-BB express benchmark BB (Hadoop based big data system), IO intensity queries. TPC-H\\nGDS for PyTorch (GDS numpy, GDS numpy DALI, GDS numpy 1-file with DeepLabv3+), on CAM5 climate data set to predict extreme weather patterns. FUTUREWEI INTERNAL\\n8\\n\\n\\nSlide #8: \\nNvidia GPUDirect Storage (GDS) support, v0.9 Nov 2020\\nFUTUREWEI INTERNAL\\n9\\nMode 1: “NVMeoF with ext4”\\nTesla GPU\\n\\nlibcufile.so\\nUser space\\n\\n\\nNvidia-fs.ko\\nKernel space\\nExt4 FS\\nNIC\\n(remote)\\n/dev/nvme1\\n\\nStorage node having front-end NVMe over Fabric block support\\nNIC\\nBlock service\\n\\nNIC\\nFile service\\nMode 2: “NFSoRDMA”\\nIB or RoCE switch\\nStorage node having front-end NFS over RDMA support\\nNFS mount\\n/dev/nvme0\\nLocal NVMe drive\\nMode 0: local NVMe drive\\n\\nCUDA 11.0\\nSource: https://developer.nvidia.com/gpudirect-storage, Key features: for local NVMe with ext4, NVMeoF with ext4, NFSoRDMA \\n\\n\\nSlide #9: \\nFUTUREWEI INTERNAL\\n10\\n\\n Image: a series of photos showing different types of electronic devices', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}]}}"
     ]
    }
   ],
   "source": [
    "query = \"What is the nvidia ecosystem?\"\n",
    "for token in ns.chat_engine.chat_stream(query):\n",
    "    print(token, end='', flush=True)  # Print each token as it's generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nvidia ecosystem refers to a broad range of products, services, and technologies developed by NVIDIA Corporation that support its graphics processing units (GPUs) and other computing hardware. The ecosystem includes:\n",
      "\n",
      "1. **GPUs**: NVIDIA's graphics processing units, which are used for gaming, professional visualization, artificial intelligence, high-performance computing, and other applications.\n",
      "2. **CUDA**: A parallel computing platform and programming model developed by NVIDIA, allowing developers to harness the power of GPUs for general-purpose computing.\n",
      "3. **Deep Learning**: NVIDIA provides tools and technologies for deep learning, including Tensor Cores, cuDNN, and Deep Learning SDKs.\n",
      "4. **Nvidia Drive**: A platform for developing autonomous vehicles, featuring GPUs, software development kits, and other technologies.\n",
      "5. **Nvidia Grid**: A cloud-based platform delivering high-performance computing resources to businesses and researchers.\n",
      "6. **Nvidia DGX**: Pre-configured systems providing a complete solution for AI, deep learning, and high-performance computing.\n",
      "7. **Nvidia Quadro**: Professional-grade GPUs designed for workstations and applications requiring high performance and reliability.\n",
      "8. **Nvidia Tesla**: Datacenter-focused GPUs designed for high-performance computing, artificial intelligence, and other applications.\n",
      "\n",
      "The ecosystem also includes software tools and technologies, such as:\n",
      "\n",
      "1. **GPU Driver**: Proprietary driver software managing interaction between NVIDIA GPUs and operating systems.\n",
      "2. **CUDA Toolkit**: Development tools and libraries for building applications on top of CUDA.\n",
      "3. **Deep Learning SDKs**: Software development kits for deep learning applications.\n",
      "4. **Tensor Cores**: Technology enabling fast matrix multiplication and other linear algebra operations on GPUs.\n",
      "\n",
      "The Nvidia ecosystem is designed to provide a comprehensive platform for developers, researchers, and businesses to build and deploy high-performance computing applications, including those related to artificial intelligence, machine learning, and data science.\n",
      "\n",
      "In the context of the provided text, it appears that the Nvidia ecosystem is being explored as part of a research project or collaboration between Futurewei and NVIDIA.\n",
      "\n",
      "Full response: The Nvidia ecosystem refers to a broad range of products, services, and technologies developed by NVIDIA Corporation that support its graphics processing units (GPUs) and other computing hardware. The ecosystem includes:\n",
      "\n",
      "1. **GPUs**: NVIDIA's graphics processing units, which are used for gaming, professional visualization, artificial intelligence, high-performance computing, and other applications.\n",
      "2. **CUDA**: A parallel computing platform and programming model developed by NVIDIA, allowing developers to harness the power of GPUs for general-purpose computing.\n",
      "3. **Deep Learning**: NVIDIA provides tools and technologies for deep learning, including Tensor Cores, cuDNN, and Deep Learning SDKs.\n",
      "4. **Nvidia Drive**: A platform for developing autonomous vehicles, featuring GPUs, software development kits, and other technologies.\n",
      "5. **Nvidia Grid**: A cloud-based platform delivering high-performance computing resources to businesses and researchers.\n",
      "6. **Nvidia DGX**: Pre-configured systems providing a complete solution for AI, deep learning, and high-performance computing.\n",
      "7. **Nvidia Quadro**: Professional-grade GPUs designed for workstations and applications requiring high performance and reliability.\n",
      "8. **Nvidia Tesla**: Datacenter-focused GPUs designed for high-performance computing, artificial intelligence, and other applications.\n",
      "\n",
      "The ecosystem also includes software tools and technologies, such as:\n",
      "\n",
      "1. **GPU Driver**: Proprietary driver software managing interaction between NVIDIA GPUs and operating systems.\n",
      "2. **CUDA Toolkit**: Development tools and libraries for building applications on top of CUDA.\n",
      "3. **Deep Learning SDKs**: Software development kits for deep learning applications.\n",
      "4. **Tensor Cores**: Technology enabling fast matrix multiplication and other linear algebra operations on GPUs.\n",
      "\n",
      "The Nvidia ecosystem is designed to provide a comprehensive platform for developers, researchers, and businesses to build and deploy high-performance computing applications, including those related to artificial intelligence, machine learning, and data science.\n",
      "\n",
      "In the context of the provided text, it appears that the Nvidia ecosystem is being explored as part of a research project or collaboration between Futurewei and NVIDIA.\n",
      "Metadata: {'sources': [{'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #0: \\nNvidia ecosystem\\x0band how Futurewei supports it\\n\\n\\n\\nSlide #1: \\nOptimization stack\\n\\n Image: a series of photos showing different types of green plants\\n\\n\\nMulti-core, multi-thread programming library that supports vector optimization. ML, DL libraries and tools based on CUDA. Bluefield DPU, MPI tag matching, Mellanox SHARP\\nTelemetry and troubleshooting across compute, network, and storage layers. Cumulus NetQ, Mellanox UFM\\nSource : https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/\\n\\n\\nSlide #3: \\nSoftware ecosystems from Nvidia\\nNGC registry\\nhttp://ngc.nvidia.com\\n\\nContainers, models, pipelines for Nvidia GPUs. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. ML, DL libraries and tools based on CUDA. APP/data engine/AIML pipeline\\nSource: https://www.nvidia.com/en-us/data-center/magnum-io/\\n\\n\\nSlide #2: \\nFUTUREWEI INTERNAL\\n3\\n\\n Image: a collage of photos showing different types of electronic devices\\n\\n\\n Image: a green and white sign on a blue wall\\n\\nThe GPU bypasses the CPU and system memory, and accesses remote storage via 8X 200 Gb/s NICs, achieving up to 1.6Terabits/s of raw storage bandwidth. GPUDirect, Mellanox NVMe SNAP\\nNVIDIA NVLink® fabric and RDMA-based network IO acceleration reduces IO overhead, bypassing the CPU and enabling direct GPU to GPU data transfers at line rates\\nDPDK, GPUDirect RDMA, HPC-X, NCCL, NVSHEM, UCX, ASAP\\nOffloading to “network processors”. APP/data engine/AIML pipeline\\nSource: https://www.nvidia.com/en-us/data-center/magnum-io/\\n\\n\\nSlide #2: \\nFUTUREWEI INTERNAL\\n3\\n\\n Image: a collage of photos showing different types of electronic devices\\n\\n\\n Image: a green and white sign on a blue wall\\n\\nThe GPU bypasses the CPU and system memory, and accesses remote storage via 8X 200 Gb/s NICs, achieving up to 1.6Terabits/s of raw storage bandwidth. GPUDirect, Mellanox NVMe SNAP\\nNVIDIA NVLink® fabric and RDMA-based network IO acceleration reduces IO overhead, bypassing the CPU and enabling direct GPU to GPU data transfers at line rates\\nDPDK, GPUDirect RDMA, HPC-X, NCCL, NVSHEM, UCX, ASAP\\nOffloading to “network processors”. Bluefield DPU, MPI tag matching, Mellanox SHARP\\nTelemetry and troubleshooting across compute, network, and storage layers. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. Slide #5: \\nTarget #2: GPU-heavy cluster (DGX-2 like) storage reference architecture (less dependent, less expensive)\\nFUTUREWEI INTERNAL\\n6\\n…\\nServers loaded with Tesla GPUs\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv Cumulus NetQ, Mellanox UFM\\nSource : https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/\\n\\n\\nSlide #3: \\nSoftware ecosystems from Nvidia\\nNGC registry\\nhttp://ngc.nvidia.com\\n\\nContainers, models, pipelines for Nvidia GPUs. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. GDS (GPU Directed Storage)\\nhttps://developer.nvidia.com/gpudirect-storage\\n\\nA GDS-compliant file system must be used. FUTUREWEI INTERNAL\\n4\\n\\n\\nSlide #4: \\nTarget #1: DGX-2 Storage reference architecture\\x0b(Depending on the collaboration with Nvidia and budget)\\nFUTUREWEI INTERNAL\\n5\\n…\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\nDGX-2 systems\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a remote control sitting on the ground\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with two DGX-2 systems. Slide #5: \\nTarget #2: GPU-heavy cluster (DGX-2 like) storage reference architecture (less dependent, less expensive)\\nFUTUREWEI INTERNAL\\n6\\n…\\nServers loaded with Tesla GPUs\\n\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nSlide #14: \\nDeliverable#2 Open-source config scripts\\nFUTUREWEI INTERNAL\\n15\\nAnsible\\ninventory\\n\\nhost1\\nhost2\\nVMs temporarily not supported\\nplaybook\\ninfra\\n\\n Image: a yellow and orange striped piece of paper\\n\\ncommon\\n\\n Image: a yellow and orange striped piece of paper\\n\\nai\\n\\n Image: a yellow and orange striped piece of paper\\n\\ndatalake\\n\\n Image: a yellow and orange striped piece of paper\\n\\nadas\\n\\n Image: a yellow and orange striped piece of paper\\n\\nhpc\\nPlan to create very basic ansible steps and test programs as open-source projects. Could release as part of project Caerus. Slide #15: \\nDeliverable#3 Nvidia Eco System Collaboration\\nExplore the possibilities to become a Nvidia partner as storage vendor\\nWorking in-progress\\nMay need company executive level supports\\n\\n\\nSlide #16: \\nDeliverable#4 Published Paper - PayU model based ADAS Solution\\nWork with EU team to have a PayU based solution\\nADAS solution based on current and near future products\\nADAS storage key requirements\\nNeed company multi-department level collaborations and supports. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch. Slide #17: \\nDeliverable#5 Published Paper for Futurewei’s Open Source Based Big Data platform solution\\nReference architecture using Huawei equipment\\nSome performance / benchmark numbers, based on open source\\nBenchmark selection\\nTuning guideline based on benchmark workload requirement for storage\\nMarket analysis for Bigdata intelligent systems. FUTUREWEI INTERNAL\\n18\\n\\n\\nSlide #18: \\n\\n\\nSlide #19: \\nAI software ecosystem\\nFUTUREWEI INTERNAL\\n20\\n\\n Image: a collage of images of a person on a cell phone\\n\\n\\n Image: an aerial photo of a group of airplanes\\n\\nIn 2020, TensorFlow has a narrow edge over PyTorch. But in research field, researcher prefer Pytorch over TensorFlow.', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nImage: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n\\nSlide #6: \\nCommon data solution reference architecture (open-source)\\nFUTUREWEI INTERNAL\\n7\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost 1\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost n\\n\\nMulti-host connected by high speed RDMA network (IB/RoCE)\\nNIC\\nNIC\\n\\nRemote storage\\nNIC\\n\\nConnect to storage with NVMe over Fabric (IB/RoCE)\\nLUNs\\nFiles\\nObjects\\nNVMe driver\\nCUDA\\nData lake\\nAI\\nADAS\\nCommon\\nPython3\\nCUDA-X\\nDocker\\nK8S\\nJava\\nGCC\\nScala\\nSpark core\\nSystem\\nConfiguration\\nSoftware\\nConfiguration\\nTensorFlow\\nPytorch\\nSpark SQL\\nHive\\nPresto\\nAlluxio\\nTPC-DS\\nBigBench v2\\nBDD 100K\\nSpark ML\\nFlink\\nSpark Stream\\nHorovod\\nRay\\nGPU driver\\nJupyter hub\\nApollo\\nCARLA\\nErdos\\nAWS\\nAzure\\nGoogle Cloud\\nHPC\\nopenHPC\\ntest-suite-ohpc\\nMySQL\\nKeras\\nNvidia container runtime\\nGPU scheduler? file_path: /mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx\\n\\nImage: a black and white photo of a black and white computer\\n\\nIB or Ethernet\\nStorage\\n\\n\\n Image: a black and white photo of a black and white computer\\n\\n\\nA quick assembly of the components will yield a rack loaded with GPU-heavy servers and storage, connected by RoCE or IB network. Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n Image: a black and white photo of a black and white tv\\n\\n\\n\\nSlide #6: \\nCommon data solution reference architecture (open-source)\\nFUTUREWEI INTERNAL\\n7\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost 1\\nGPU\\nGPU\\n\\nMulti-GPU NVlink\\n\\nHost n\\n\\nMulti-host connected by high speed RDMA network (IB/RoCE)\\nNIC\\nNIC\\n\\nRemote storage\\nNIC\\n\\nConnect to storage with NVMe over Fabric (IB/RoCE)\\nLUNs\\nFiles\\nObjects\\nNVMe driver\\nCUDA\\nData lake\\nAI\\nADAS\\nCommon\\nPython3\\nCUDA-X\\nDocker\\nK8S\\nJava\\nGCC\\nScala\\nSpark core\\nSystem\\nConfiguration\\nSoftware\\nConfiguration\\nTensorFlow\\nPytorch\\nSpark SQL\\nHive\\nPresto\\nAlluxio\\nTPC-DS\\nBigBench v2\\nBDD 100K\\nSpark ML\\nFlink\\nSpark Stream\\nHorovod\\nRay\\nGPU driver\\nJupyter hub\\nApollo\\nCARLA\\nErdos\\nAWS\\nAzure\\nGoogle Cloud\\nHPC\\nopenHPC\\ntest-suite-ohpc\\nMySQL\\nKeras\\nNvidia container runtime\\nGPU scheduler? Prometheus\\nGrafana\\nKafka\\nArgo workflow\\nGPU monitoring\\nVertical (ADAS, BIO) training\\nSimulation\\nNFS\\n\\nExtended solution\\n\\n\\n\\nSlide #7: \\nBenchmark selection/adoption\\nNeed to have storage needs (large data sets, IO bound, etc)\\nCandidates\\nBDD100k (ADAS related) (2TB)\\nSimulation and visualizing NASA Mars Lander data set (128TB)\\nSimulate, analyze, and visualize molecular dynamics dataset (17TB)\\nTPCx-BB express benchmark BB (Hadoop based big data system), IO intensity queries. TPC-H\\nGDS for PyTorch (GDS numpy, GDS numpy DALI, GDS numpy 1-file with DeepLabv3+), on CAM5 climate data set to predict extreme weather patterns. FUTUREWEI INTERNAL\\n8\\n\\n\\nSlide #8: \\nNvidia GPUDirect Storage (GDS) support, v0.9 Nov 2020\\nFUTUREWEI INTERNAL\\n9\\nMode 1: “NVMeoF with ext4”\\nTesla GPU\\n\\nlibcufile.so\\nUser space\\n\\n\\nNvidia-fs.ko\\nKernel space\\nExt4 FS\\nNIC\\n(remote)\\n/dev/nvme1\\n\\nStorage node having front-end NVMe over Fabric block support\\nNIC\\nBlock service\\n\\nNIC\\nFile service\\nMode 2: “NFSoRDMA”\\nIB or RoCE switch\\nStorage node having front-end NFS over RDMA support\\nNFS mount\\n/dev/nvme0\\nLocal NVMe drive\\nMode 0: local NVMe drive\\n\\nCUDA 11.0\\nSource: https://developer.nvidia.com/gpudirect-storage, Key features: for local NVMe with ext4, NVMeoF with ext4, NFSoRDMA \\n\\n\\nSlide #9: \\nFUTUREWEI INTERNAL\\n10\\n\\n Image: a series of photos showing different types of electronic devices', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Nvidia ecosystem.pptx', 'file_name': 'Nvidia ecosystem.pptx', 'file_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'file_size': 4084256, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Print each token as it's generated\n",
    "response_generator = ns.chat_engine.chat_stream(query)\n",
    "for item in response_generator:\n",
    "    if isinstance(item, str):\n",
    "        print(item, end='', flush=True)\n",
    "    else:\n",
    "        # This is the final yield with the full response and metadata\n",
    "        full_response = item\n",
    "        break\n",
    "\n",
    "print(\"\\n\\nFull response:\", full_response['response'])\n",
    "print(\"Metadata:\", full_response['metadata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:39:38,122 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-07 15:39:38,124 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "2024-10-07 15:39:48,079 - nexusync.core.indexer - INFO - Loaded 8 files from ../sample_docs\n",
      "2024-10-07 15:39:48,080 - nexusync.core.indexer - INFO - Updated 0 files in ../sample_docs\n",
      "2024-10-07 15:39:48,081 - nexusync.core.indexer - INFO - Deleted file: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\n",
      "2024-10-07 15:39:48,082 - nexusync.core.indexer - INFO - Deleting 1 files from the index.\n",
      "2024-10-07 15:39:48,083 - nexusync.core.indexer - INFO - Deletion process completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add a new document\n",
    "with open(\"../sample_docs/new_added.txt\", \"w\") as f:\n",
    "    f.write(\"Breaking News: Trump and Harris had a fight!\")\n",
    "\n",
    "# Refresh the index: incremental in new files and detect deleted files in the folder\n",
    "ns.refresh_index()\n",
    "print(\"Index refreshed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: whhat news about tesla?\n",
      "Response: Based on the provided context, here's the answer:\n",
      "\n",
      "Tesla has released software update 2024.38 for its employees, which includes two main features: \n",
      "\n",
      "1. The ability to use Spotify with a free Spotify account (still requiring Premium Connectivity).\n",
      "2. Improvements to the vehicle’s side mirror functions.\n",
      "\n",
      "Additionally, Tesla now allows users to fold their side mirrors in and out using the quick menu, and there are also minor fixes and improvements in the update.\n",
      "Response: {'sources': [{'source_text': \"file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nThe Features in Tesla's 2024.38 Software Update\\n\\nOctober 4, 2024\\n\\nBy Karan Singh\\n\\n\\n\\n\\n\\nNot a Tesla App\\n\\nTesla has released\\xa0software update 2024.38\\xa0to its employees for its last round of internal testing. We now have preliminary release notes for this update. You’ll also be able to unfold them from the quick menu as well. Other Updates\\n\\nSimilar to other recent updates, Tesla now has an Other Updates section where they list smaller features that don’t warrant their own release notes. In this section, Tesla outlines four improvements, although only two are noteworthy. One of them is the ability to manually fold the side mirrors using the quick function that we talked about above and the other is improvements to the WiFi menu. The four points Tesla mentions in their release notes are:\\n\\n- The Wi-Fi menu provides more information about your Wi-Fi connection as well as tips for improving your connection. - Hold down the left scroll wheel and select Fold Mirrors to fold the mirrors in or out. Spotify\\n\\nSince\\xa0Premium Connectivity’s free Spotify Premium\\xa0(for Europeans only) is going the way of the dodo, it looks like Tesla and Spotify will now allow you to log in to your Spotify account with a free account, at least in Europe and Australia. Tesla also recently announced that vehicles in North America that use the “Streaming” app will\\xa0now be changed to a free version of Slacker\\xa0that will include ads and other limitations. If the ability to use Spotify for free is limited to Europe and Australia, where these users already had free access to Spotify Premium, then this isn’t a huge deal. We’ve recently been able to confirm these changes with an employee. Spotify\\n\\nSince\\xa0Premium Connectivity’s free Spotify Premium\\xa0(for Europeans only) is going the way of the dodo, it looks like Tesla and Spotify will now allow you to log in to your Spotify account with a free account, at least in Europe and Australia. Tesla also recently announced that vehicles in North America that use the “Streaming” app will\\xa0now be changed to a free version of Slacker\\xa0that will include ads and other limitations. If the ability to use Spotify for free is limited to Europe and Australia, where these users already had free access to Spotify Premium, then this isn’t a huge deal. However, if Tesla and Spotify now allow you to stream Spotify in your vehicle with a free account, that will be a huge win for many owners. The free version of Spotify has limited skips and includes ads. - Hold down the left scroll wheel and select Fold Mirrors to fold the mirrors in or out. - This update contains important security fixes and improvements\\n\\n- This version contains minor fixes and improvements\\n\\n\\n\\nNvidia CEO's bombshell raises the bar for the stock\\n\\nSilin Chen\\n\\nSat, October 5, 2024 at 2:03 PM GMT+1\\xa03 min read\\n\\n35\\n\\nIn This Article:\\n\\nStockStory Top Pick Other Updates\\n\\nSimilar to other recent updates, Tesla now has an Other Updates section where they list smaller features that don’t warrant their own release notes. In this section, Tesla outlines four improvements, although only two are noteworthy. One of them is the ability to manually fold the side mirrors using the quick function that we talked about above and the other is improvements to the WiFi menu. Other music services require you to have a premium or paid subscription before allowing you to stream in the vehicle. In either case, you’ll still need Tesla’s Premium Connectivity plan to stream music over the vehicle’s connection, or you could use your phone’s hotspot feature. Side Mirrors\\n\\nThe side mirrors are finally getting some additional customization options with two improvements. First, you will now be able to customize the side mirror’s tilt when reversing. By default, a Tesla’s side mirrors will angle downwards while reversing to provide a better perspective of what’s close to the edges of your vehicle, rather than your blind spots. You’ll now be able to customize the level of tilt when this happens. This will help folks who are either taller or shorter than the norm – you’ll now be able to customize the angles to better suit you. It’s a simple update, but a much-appreciated one! Next up for the side mirrors – you’ll be able to fold them in the quick menu. file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nThe Features in Tesla's 2024.38 Software Update\\n\\nOctober 4, 2024\\n\\nBy Karan Singh\\n\\n\\n\\n\\n\\nNot a Tesla App\\n\\nTesla has released\\xa0software update 2024.38\\xa0to its employees for its last round of internal testing. We now have preliminary release notes for this update. It looks like this update will be smaller and focus on two main changes. However, keep in mind that there could be additional features that may only apply to certain regions or vehicles that are not mentioned here. The two features are the ability to use Spotify with a free Spotify account (still requiring Premium Connectivity), and improvements to the vehicle’s side mirror functions. We’ve recently been able to confirm these changes with an employee. Next up for the side mirrors – you’ll be able to fold them in the quick menu. That’s the menu that pops up when you press and hold down the left scroll wheel. You’ll also be able to unfold them from the quick menu as well.\", 'metadata': {'file_name': 'news.docx', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/news.docx', 'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'file_size': 298730, 'creation_date': '2024-10-07', 'last_modified_date': '2024-10-07'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/new_added.txt\\n\\nBreaking News: Trump and Harris had a fight!', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/new_added.txt', 'file_name': 'new_added.txt', 'file_type': 'text/plain', 'file_size': 44, 'creation_date': '2024-10-07', 'last_modified_date': '2024-10-07'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nNvidia CEO\\'s bombshell raises the bar for the stock\\n\\nSilin Chen\\n\\nSat, October 5, 2024 at 2:03 PM GMT+1\\xa03 min read\\n\\n35\\n\\nIn This Article:\\n\\nStockStory Top Pick\\n\\n\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\nThink of how Nvidia’s GeForce 8800 chip, launched in 2006, changed the gaming landscape. Now, almost two decades later, Nvidia is still making that progression, with its Blackwell designed to change the world of artificial intelligence. Story Continues\\n\\nView Comments\\xa0(35)\\n\\nTerms\\n\\n\\xa0and\\xa0\\n\\nPrivacy Policy\\n\\nPrivacy & Cookie Settings\\n\\n\\n\\n\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia Chief Financial Officer Colette Kress said during the August earnings call. Story Continues\\n\\nView Comments\\xa0(35)\\n\\nTerms\\n\\n\\xa0and\\xa0\\n\\nPrivacy Policy\\n\\nPrivacy & Cookie Settings\\n\\n\\n\\n\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC “The demand for Blackwell is insane. Everybody wants to have the most, and everybody wants to be first.”\\n\\n\\n\\nNvidia\\'s stock has surged by over 150% this year, following an impressive 240% gain in 2023. Hyperscaler buyers like Amazon\\xa0\\xa0(AMZN)\\xa0, Microsoft\\xa0\\xa0(MSFT)\\xa0, and Alphabet\\xa0\\xa0(GOOGL)\\xa0\\xa0are expected to spend around $160 billion in 2024 on AI infrastructure, according to Bernstein analysts. Nvidia forecasts $32.5 billion in revenue for the current quarter, an 80% increase from last year. Related: Veteran trader targets Nvidia as shares slide\\n\\nNvidia plans to ship Blackwell GPUs to clients in Q4 of this year, with a consumer release expected in 2025. “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia Chief Financial Officer Colette Kress said during the August earnings call. “If we can increase the performance, like we\\'ve done for Hopper and Blackwell ... we\\'re effectively increasing the revenue or throughput for our customers on these infrastructures by a couple to three times each year,\" Huang added. Nvidia\\'s financial performance exceeds expectations\\n\\nNvidia’s latest earnings report further solidifies its strong position in the AI market. On August 28, the company posted earnings per share of 68 cents, beating Wall Street expectations of 64 cents. “Blackwell is in full production,” Huang said in an interview with CNBC. “The demand for Blackwell is insane. Everybody wants to have the most, and everybody wants to be first.”\\n\\n\\n\\nNvidia\\'s stock has surged by over 150% this year, following an impressive 240% gain in 2023. These large language models are trained on extensive datasets to understand and generate responses in human language. “Blackwell is in full production,” Huang said in an interview with CNBC. “The demand for Blackwell is insane. The cost of Blackwell is expected to range between $30,000 and $40,000 per unit. Huang emphasized the importance of continuous updates to Nvidia’s AI infrastructure, with the company releasing new platforms annually. “If we can increase the performance, like we\\'ve done for Hopper and Blackwell ... we\\'re effectively increasing the revenue or throughput for our customers on these infrastructures by a couple to three times each year,\" Huang added. Now, almost two decades later, Nvidia is still making that progression, with its Blackwell designed to change the world of artificial intelligence. Demand is \"insane,\" Nvidia’s chief executive Jensen Huang recently said. Major cloud providers like AWS, Azure, and Google Cloud are integrating Blackwell into their infrastructure to support high-performance AI workloads. Related: Nvidia CEO Jensen Huang just told investors what’s next for the AI chipmaker\\n\\nOracle\\xa0announced\\xa0on October 2 that it would need 131,072 Nvidia Blackwell GPUs as part of a $6.5 billion investment to establish a new public cloud region in Malaysia, another proof of a strong need for advanced AI processing capabilities. Blackwell is a platform Nvidia launched in March that allows organizations to run real-time generative AI on models with trillions of parameters. These large language models are trained on extensive datasets to understand and generate responses in human language.', 'metadata': {'file_name': 'news.docx', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/news.docx', 'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'file_size': 298730, 'creation_date': '2024-10-07', 'last_modified_date': '2024-10-07'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"whhat news about tesla?\"\n",
    "text_qa_template = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information above, I want you to think step by step to answer the query in a crisp manner. \"\n",
    "    \"In case you don't know the answer, say 'I don't know!'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "response = ns.query(text_qa_template = text_qa_template, query = query )\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "print(f\"Response: {response['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:40:27,349 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-07 15:40:27,353 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-07 15:40:36,886 - nexusync.core.indexer - INFO - Loaded 7 files from ../sample_docs\n",
      "2024-10-07 15:40:36,887 - nexusync.core.indexer - INFO - Updated 0 files in ../sample_docs\n",
      "2024-10-07 15:40:36,888 - nexusync.core.indexer - INFO - No deleted files found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed after deletion.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Delete the new document\n",
    "# os.remove('../sample_docs/Nvidia ecosystem.pptx')\n",
    "# print(\"New document deleted.\")\n",
    "\n",
    "ns.refresh_index()\n",
    "print(\"Index refreshed after deletion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is the breaking news?\n",
      "Response: Based on the provided context, it appears that there are two main breaking news stories:\n",
      "\n",
      "1. Nvidia's CEO Jensen Huang has mentioned that the demand for Nvidia's Blackwell technology is \"insane\" and that everybody wants to have the most and be first.\n",
      "2. Nvidia's latest earnings report showed strong financial performance, with revenue hitting $30.04 billion, up 122%, and beating Wall Street expectations.\n",
      "\n",
      "However, without more specific information, it's difficult to pinpoint a single breaking news story.\n",
      "Response: {'sources': [{'source_text': \"file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC\\n\\n\\n\\nPalantir\\xa0Technologies\\xa0(NYSE: PLTR)\\xa0and\\xa0Nvidia\\xa0(NASDAQ: NVDA)\\xa0are two of the hottest\\xa0artificial intelligence (AI) stocks\\xa0on Wall Street. Suffice it to say Wall Street is overwhelmingly bearish on Palantir but very bullish on Nvidia. Here are the most important details for investors. Palantir Technologies: 32% downside implied by the median price target\\n\\nPalantir has deep roots in counterterrorism and clandestine military operations. Among the 65 analysts following Nvidia, the median price target is $150 per share, which implies 20% upside from the current share price of $125. Furthermore, Palantir is the most overvalued stock in the S&P 500 based on the difference between its current price and median price target. Meanwhile, according to\\xa0FactSet Research, Nvidia ranks among the most highly recommended stocks in the S&P 500 in terms of its percentage of buy ratings. file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC\\n\\n\\n\\nPalantir\\xa0Technologies\\xa0(NYSE: PLTR)\\xa0and\\xa0Nvidia\\xa0(NASDAQ: NVDA)\\xa0are two of the hottest\\xa0artificial intelligence (AI) stocks\\xa0on Wall Street. In fact, with year-to-date returns of 132% and 150%, respectively, they rank among the five best-performing components of the\\xa0S&P 500. However, Wall Street expects the stocks to move in opposite directions over the next year. Among the 23 analysts who follow Palantir, the median price target is $27 per share, which implies 32% downside from its current share price of $40. Among the 65 analysts following Nvidia, the median price target is $150 per share, which implies 20% upside from the current share price of $125. Among the 23 analysts who follow Palantir, the median price target is $27 per share, which implies 32% downside from its current share price of $40. Among the 65 analysts following Nvidia, the median price target is $150 per share, which implies 20% upside from the current share price of $125. Furthermore, Palantir is the most overvalued stock in the S&P 500 based on the difference between its current price and median price target. Meanwhile, according to\\xa0FactSet Research, Nvidia ranks among the most highly recommended stocks in the S&P 500 in terms of its percentage of buy ratings. Suffice it to say Wall Street is overwhelmingly bearish on Palantir but very bullish on Nvidia. Here are the most important details for investors. Furthermore, Palantir is the most overvalued stock in the S&P 500 based on the difference between its current price and median price target. Meanwhile, according to\\xa0FactSet Research, Nvidia ranks among the most highly recommended stocks in the S&P 500 in terms of its percentage of buy ratings. Suffice it to say Wall Street is overwhelmingly bearish on Palantir but very bullish on Nvidia. Some analysts have lauded Palantir for its sophisticated technology. For instance, it was a top-ranked vendor in Dresner Advisory Services' 2024 market study on artificial intelligence, data science, and machine learning platforms. Forrester Research\\xa0recently recognized its leadership in AI and machine learning platforms. That means some clients find Palantir's software so complex that they struggle to use it independently. Gartner also omitted Palantir in its latest report on data science and machine learning platforms. Forrester Research\\xa0recently recognized its leadership in AI and machine learning platforms. Other analysts are less impressed. Gartner\\xa0scored Palantir below a dozen other vendors in data integration capabilities, citing overreliance on consulting services.\", 'metadata': {'file_name': 'news.docx', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/news.docx', 'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'file_size': 274441, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/news.docx\\n\\nNvidia CEO\\'s bombshell raises the bar for the stock\\n\\nSilin Chen\\n\\nSat, October 5, 2024 at 2:03 PM GMT+1\\xa03 min read\\n\\n35\\n\\nIn This Article:\\n\\nStockStory Top Pick\\n\\n\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\nThink of how Nvidia’s GeForce 8800 chip, launched in 2006, changed the gaming landscape. Now, almost two decades later, Nvidia is still making that progression, with its Blackwell designed to change the world of artificial intelligence. “Blackwell is in full production,” Huang said in an interview with CNBC. “The demand for Blackwell is insane. Everybody wants to have the most, and everybody wants to be first.”\\n\\n\\n\\nNvidia\\'s stock has surged by over 150% this year, following an impressive 240% gain in 2023. “The demand for Blackwell is insane. Everybody wants to have the most, and everybody wants to be first.”\\n\\n\\n\\nNvidia\\'s stock has surged by over 150% this year, following an impressive 240% gain in 2023. Hyperscaler buyers like Amazon\\xa0\\xa0(AMZN)\\xa0, Microsoft\\xa0\\xa0(MSFT)\\xa0, and Alphabet\\xa0\\xa0(GOOGL)\\xa0\\xa0are expected to spend around $160 billion in 2024 on AI infrastructure, according to Bernstein analysts. “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia Chief Financial Officer Colette Kress said during the August earnings call. Story Continues\\n\\nView Comments\\xa0(35)\\n\\nTerms\\n\\n\\xa0and\\xa0\\n\\nPrivacy Policy\\n\\nPrivacy & Cookie Settings\\n\\n\\n\\n\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC Story Continues\\n\\nView Comments\\xa0(35)\\n\\nTerms\\n\\n\\xa0and\\xa0\\n\\nPrivacy Policy\\n\\nPrivacy & Cookie Settings\\n\\n\\n\\n\\n\\nPalantir Stock vs. Nvidia Stock: Wall Street Says Sell One and Buy the Other\\n\\n\\n\\nTrevor Jennewine, The Motley Fool\\n\\nSun, October 6, 2024 at 8:55 AM GMT+1\\xa05 min read\\n\\n18\\n\\nIn This Article:\\n\\n\\n\\nNVDA\\n\\n+1.68%\\n\\n\\n\\nPLTR\\n\\n\\n\\n\\n\\n^GSPC “If we can increase the performance, like we\\'ve done for Hopper and Blackwell ... we\\'re effectively increasing the revenue or throughput for our customers on these infrastructures by a couple to three times each year,\" Huang added. Nvidia\\'s financial performance exceeds expectations\\n\\nNvidia’s latest earnings report further solidifies its strong position in the AI market. On August 28, the company posted earnings per share of 68 cents, beating Wall Street expectations of 64 cents. On August 28, the company posted earnings per share of 68 cents, beating Wall Street expectations of 64 cents. Revenue hit $30.04 billion, up 122%, surpassing the anticipated $28.7 billion. Nvidia forecasts $32.5 billion in revenue for the current quarter, an 80% increase from last year. These large language models are trained on extensive datasets to understand and generate responses in human language. “Blackwell is in full production,” Huang said in an interview with CNBC. “The demand for Blackwell is insane. Now, almost two decades later, Nvidia is still making that progression, with its Blackwell designed to change the world of artificial intelligence. Demand is \"insane,\" Nvidia’s chief executive Jensen Huang recently said. Major cloud providers like AWS, Azure, and Google Cloud are integrating Blackwell into their infrastructure to support high-performance AI workloads. Nvidia\\'s financial performance exceeds expectations\\n\\nNvidia’s latest earnings report further solidifies its strong position in the AI market. On August 28, the company posted earnings per share of 68 cents, beating Wall Street expectations of 64 cents. Revenue hit $30.04 billion, up 122%, surpassing the anticipated $28.7 billion. Nvidia forecasts $32.5 billion in revenue for the current quarter, an 80% increase from last year. Related: Veteran trader targets Nvidia as shares slide\\n\\nNvidia plans to ship Blackwell GPUs to clients in Q4 of this year, with a consumer release expected in 2025. “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia Chief Financial Officer Colette Kress said during the August earnings call.', 'metadata': {'file_name': 'news.docx', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/news.docx', 'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'file_size': 274441, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}, {'source_text': 'file_path: /mnt/d/nexusync/notebooks/../sample_docs/hdiifye2023.xlsx\\n\\n3 33.8 7\\n2012/13 5.5 4.1 1.4 34.4 6.8\\n2013/14 5.8 4.2 1.5 35.3 8.2\\n2014/15 5.7 4.2 1.4 34.7 7\\n2015/16 5.6 3.9 1.4 35.1 8.5\\n2016/17 5.2 3.9 1.3 33.4 7\\n2017/18 5.8 4.2 1.4 35 7.7\\n2018/19 6.2 4.6 1.5 36 8\\n2019/20 5.8 4.3 1.4 35.4 8.2\\n2020/21 5.9 4.3 1.4 34.4 7.4\\n2021/22 6.2 4.5 1.5 35.5 8.1\\n2022/23 5.5 4.2 1.3 33.1 6.7\\nBack to index     \\nPlease click to email us your opinion: This met my needs, please produce it next year I need something slightly different (please specify) This is not what I need at all (please specify)', 'metadata': {'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/hdiifye2023.xlsx', 'file_name': 'hdiifye2023.xlsx', 'file_type': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'file_size': 306335, 'creation_date': '2024-10-06', 'last_modified_date': '2024-10-06'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the breaking news?\"\n",
    "text_qa_template = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information above, I want you to think step by step to answer the query in a crisp manner. \"\n",
    "    \"In case you don't know the answer, say 'I don't know!'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "response = ns.query(text_qa_template = text_qa_template, query = query )\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "print(f\"Response: {response['metadata']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3.2-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
