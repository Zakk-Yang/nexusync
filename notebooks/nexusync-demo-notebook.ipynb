{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NexuSync Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use NexuSync for document indexing, querying, and other key functionalities.\n",
    "\n",
    "NexuSync is a powerful library designed for efficient document indexing and querying, using state-of-the-art language and embedding models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NexuSync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing NexuSync with the required parameters. Adjust the following parameters according to your use case:\n",
    "\n",
    "- **input_dirs**: List of directories containing documents for indexing.\n",
    "- **openai_model_yn**: Whether to use OpenAI models for embeddings and language tasks.\n",
    "- **embedding_model**: Model to be used for generating embeddings.\n",
    "- **language_model**: Model to be used for language tasks.\n",
    "- **chroma_db_dir**: Directory for storing ChromaDB files.\n",
    "- **index_persist_dir**: Directory for persisting the index.\n",
    "- **chunk_size**: Size of the text chunks to be used for creating embeddings.\n",
    "- **chunk_overlap**: Overlap between text chunks to maintain context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zy-wsl/miniconda3/envs/nexusync/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-20 14:43:35,719 - nexusync.utils.embedding_models.set_embedding_model - INFO - Using HuggingFace embedding model: BAAI/bge-base-en-v1.5\n",
      "2024-10-20 14:43:35,746 - nexusync.utils.embedding_models.set_language_model - INFO - Ollama LLM initialized with model: llama3.2 and base_url: http://localhost:11434\n",
      "2024-10-20 14:43:35,824 - nexusync.NexuSync - INFO - Vectors and Querier initialized successfully.\n",
      "2024-10-20 14:43:35,827 - nexusync.core.indexer - WARNING - Index not found. Building a new index.\n",
      "2024-10-20 14:43:37,871 - nexusync.core.indexer - INFO - Loaded 2 from all directories.\n",
      "2024-10-20 14:43:40,042 - nexusync.core.indexer - INFO - Index Built.\n"
     ]
    }
   ],
   "source": [
    "from nexusync import NexuSync\n",
    "\n",
    "OPENAI_MODEL_YN = False # if False, you will use ollama model\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\" # suggested embedding model\n",
    "LANGUAGE_MODEL = \"llama3.2\" # you need to download ollama model first, please check https://ollama.com/download\n",
    "BASE_URL = \"http://localhost:11434\" # you can swith to different base_url for Ollama model\n",
    "TEMPERATURE = 0.4 # range from 0 to 1, higher means higher creativitiy level\n",
    "CHROMA_DB_DIR = 'chroma_db'\n",
    "INDEX_PERSIST_DIR = 'index_storage'\n",
    "CHROMA_COLLECTION_NAME = 'my_collection'\n",
    "INPUT_DIRS = [\"../sample_docs\"] # can specify multiple document paths\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 20\n",
    "RECURSIVE = True\n",
    "\n",
    "ns = NexuSync(input_dirs=INPUT_DIRS, \n",
    "              openai_model_yn=False, \n",
    "              embedding_model=EMBEDDING_MODEL, \n",
    "              language_model=LANGUAGE_MODEL, \n",
    "              base_url = BASE_URL,\n",
    "              temperature=TEMPERATURE, \n",
    "              chroma_db_dir = CHROMA_DB_DIR,\n",
    "              index_persist_dir = INDEX_PERSIST_DIR,\n",
    "              chroma_collection_name=CHROMA_COLLECTION_NAME,\n",
    "              chunk_overlap=CHUNK_OVERLAP,\n",
    "              chunk_size=CHUNK_SIZE,\n",
    "              recursive=RECURSIVE\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ns.start_query` can quickly test for a query with no memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 14:46:55,072 - nexusync.NexuSync - INFO - Starting query: main result of the paper can llm generate novltive ideas\n",
      "2024-10-20 14:46:58,224 - nexusync.NexuSync - INFO - Query completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: main result of the paper can llm generate novltive ideas\n",
      "Response: Based on the provided context, it appears that the paper is investigating whether Large Language Models (LLMs) can generate novel research ideas comparable to expert humans. The authors conducted a large-scale human study with over 100 NLP researchers and compared their generated ideas with those of an LLM ideation agent.\n",
      "\n",
      "The main result of the paper suggests that LLM-generated ideas are judged as more novel than human expert ideas, but slightly weaker on feasibility (p<0.05). This implies that while LLMs can generate novel ideas, they may not always be feasible or practical for actual research projects.\n",
      "\n",
      "Therefore, the answer to the query \"Can LLMs Generate Novel Research Ideas?\" is: Yes, LLMs can generate novel research ideas, but their feasibility and practicality may vary.\n",
      "Response: {'sources': [{'source_text': 'page_label: 5\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\n2.AI Ideas : Idea proposals generated by our LLM agent. We directly take the top-ranked ideas\\nfrom the agent’s output. The first author of\\nthis paper manually selected the top-ranked ideas out of all the LLM agent’s generations rather\\nthan relying on the LLM ranker in order to better estimate the upper-bound quality of AI ideas. In the next two sections, we instantiate how our LLM agent generates ideas and how our expert\\nparticipants generate and review the ideas. 3 Idea Generation Agent\\nWe build a simple but effective LLM ideation agent to compare with the human expert baseline. This aligns with existing results showing that scaling inference\\ncompute with repeated sampling can boost LLM performance on various coding and reasoning\\ntasks (Brown et al., 2024, Li et al., 2022). Specifically, we prompt the LLM to generate 4000 seed\\nideas on each research topic. The idea generation prompt includes the demonstration examples and\\nthe retrieved papers. We then use the LLM to score and rerank all retrieved papers based on three\\ncriteria: 1) the paper should be directly relevant to the specified topic; 2) the paper should be an\\nempirical paper involving computational experiments;33) the paper is interesting and can inspire\\nnew projects. The LLM is prompted to score each retrieved paper on a scale of 1 to 10 based on these\\ncriteria and we use the top-ranked papers for the next step of idea generation. 3.2 Idea Generation\\nOur key insight for idea generation is to generate as many candidate ideas as possible. To do so, we lever-\\nage retrieval-augmented generation (RAG), which has demonstrated effectiveness on many\\nknowledge-intensive tasks (Lewis et al., 2020, Shi et al., 2024). Concretely, given a research topic\\n(e.g., “novel prompting methods that can improve factuality and reduce hallucination of large\\nlanguage models\"), we prompt an LLM to generate a sequence of function calls to the Semantic\\nScholar API. We use claude-3-5-sonnet-20240620 as the backbone model for our agent but\\nthe pipeline should generalize to other LLMs as well. We keep the top k=20\\npapers from each executed function call and stop the action generation when a max of N=120 papers\\nhave been retrieved. We then use the LLM to score and rerank all retrieved papers based on three\\ncriteria: 1) the paper should be directly relevant to the specified topic; 2) the paper should be an\\nempirical paper involving computational experiments;33) the paper is interesting and can inspire\\nnew projects. The LLM is prompted to score each retrieved paper on a scale of 1 to 10 based on these\\ncriteria and we use the top-ranked papers for the next step of idea generation. Our research ideation agent\\nhas three essential components: paper retrieval, idea generation, and idea ranking, which we will\\ndescribe in detail below. 3.1 Paper Retrieval for RAG\\nTo ground idea generation, the agent needs to retrieve papers related to the given research\\ntopic, so that it will be aware of related works when generating new ideas. To do so, we lever-\\nage retrieval-augmented generation (RAG), which has demonstrated effectiveness on many\\nknowledge-intensive tasks (Lewis et al., 2020, Shi et al., 2024). In the next two sections, we instantiate how our LLM agent generates ideas and how our expert\\nparticipants generate and review the ideas. 3 Idea Generation Agent\\nWe build a simple but effective LLM ideation agent to compare with the human expert baseline. Rather than focusing on innovating the agent itself, we adhere to a minimalist design principle,\\naiming to understand the current capabilities of LLMs in idea generation. Specifically, we prompt the LLM to generate 4000 seed\\nideas on each research topic. The idea generation prompt includes the demonstration examples and\\nthe retrieved papers. We craft k= 6demonstration examples by manually summarizing exemplar\\n3Note that we exclude position papers, survey papers, and analysis papers throughout this study since their evaluation\\ntends to be very subjective. Our intuition\\nis that only a small fraction of all generated ideas might be high-quality, and we should be willing\\nto expend inference-time compute to generate more candidates so that we can later use a reranker to\\ndiscover the \"diamond in the rough\". This aligns with existing results showing that scaling inference\\ncompute with repeated sampling can boost LLM performance on various coding and reasoning\\ntasks (Brown et al., 2024, Li et al., 2022). Specifically, we prompt the LLM to generate 4000 seed\\nideas on each research topic. 3 Idea Generation Agent\\nWe build a simple but effective LLM ideation agent to compare with the human expert baseline. Rather than focusing on innovating the agent itself, we adhere to a minimalist design principle,\\naiming to understand the current capabilities of LLMs in idea generation. Our research ideation agent\\nhas three essential components: paper retrieval, idea generation, and idea ranking, which we will\\ndescribe in detail below. The LLM is prompted to score each retrieved paper on a scale of 1 to 10 based on these\\ncriteria and we use the top-ranked papers for the next step of idea generation. 3.2 Idea Generation\\nOur key insight for idea generation is to generate as many candidate ideas as possible. Our intuition\\nis that only a small fraction of all generated ideas might be high-quality, and we should be willing\\nto expend inference-time compute to generate more candidates so that we can later use a reranker to\\ndiscover the \"diamond in the rough\".', 'metadata': {'page_label': '5', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}, {'source_text': 'page_label: 1\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nCan LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers\\nChenglei Si, Diyi Yang, Tatsunori Hashimoto\\nStanford University\\n{clsi, diyiy, thashim}@stanford.edu\\nAbstract\\nRecent advancements in large language models (LLMs) have sparked optimism about their potential to\\naccelerate scientific discovery, with a growing number of works proposing research agents that autonomously\\ngenerate and validate new ideas. We address this by\\nestablishing an experimental design that evaluates research idea generation while controlling for confounders\\nand performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas,\\nwe obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we\\nfind LLM-generated ideas are judged as more novel ( p<0.05) than human expert ideas while being judged\\nslightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and\\nevaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. While these are useful applications that can potentially increase the productivity of researchers,\\nit remains an open question whether LLMs can take on the more creative and challenging parts of\\nthe research process. We focus on this problem of measuring the research ideation capabilities of LLMs and ask: are current\\nLLMs capable of generating novel ideas that are comparable to expert humans? Although ideation\\nis only one part of the research process, this is a key question to answer, as it is the very first step to the\\nscientific research process and serves as a litmus test for the possibility of autonomous research agents\\nthat create their own ideas. page_label: 1\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nCan LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers\\nChenglei Si, Diyi Yang, Tatsunori Hashimoto\\nStanford University\\n{clsi, diyiy, thashim}@stanford.edu\\nAbstract\\nRecent advancements in large language models (LLMs) have sparked optimism about their potential to\\naccelerate scientific discovery, with a growing number of works proposing research agents that autonomously\\ngenerate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very\\nfirst step of producing novel, expert-level ideas, let alone perform the entire research process. Studying our agent baselines closely, we identify open problems in building and\\nevaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose\\nan end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to\\nstudy whether these novelty and feasibility judgements result in meaningful differences in research outcome.1\\n1 Introduction\\nThe rapid improvement of LLMs, especially in capabilities like knowledge and reasoning, has enabled\\nmany new applications in scientific tasks, such as solving challenging mathematical problems (Trinh\\net al., 2024), assisting scientists in writing proofs (Collins et al., 2024), retrieving related works (Ajith\\net al., 2024, Press et al., 2024), generating code to solve analytical or computational tasks (Huang et al.,\\n2024, Tian et al., 2024), and discovering patterns in large text corpora (Lam et al., 2024, Zhong et al.,\\n2023). While these are useful applications that can potentially increase the productivity of researchers,\\nit remains an open question whether LLMs can take on the more creative and challenging parts of\\nthe research process. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose\\nan end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to\\nstudy whether these novelty and feasibility judgements result in meaningful differences in research outcome.1\\n1 Introduction\\nThe rapid improvement of LLMs, especially in capabilities like knowledge and reasoning, has enabled\\nmany new applications in scientific tasks, such as solving challenging mathematical problems (Trinh\\net al., 2024), assisting scientists in writing proofs (Collins et al., 2024), retrieving related works (Ajith\\net al., 2024, Press et al., 2024), generating code to solve analytical or computational tasks (Huang et al.,\\n2024, Tian et al., 2024), and discovering patterns in large text corpora (Lam et al., 2024, Zhong et al.,\\n2023). While these are useful applications that can potentially increase the productivity of researchers,\\nit remains an open question whether LLMs can take on the more creative and challenging parts of\\nthe research process. We focus on this problem of measuring the research ideation capabilities of LLMs and ask: are current\\nLLMs capable of generating novel ideas that are comparable to expert humans? Despite this, no evaluations have shown that LLM systems can take the very\\nfirst step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by\\nestablishing an experimental design that evaluates research idea generation while controlling for confounders\\nand performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas,\\nwe obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we\\nfind LLM-generated ideas are judged as more novel ( p<0.05) than human expert ideas while being judged\\nslightly weaker on feasibility.', 'metadata': {'page_label': '1', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}, {'source_text': 'page_label: 36\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nF Idea Generation Agent: Additional Implementation Details\\nSeed Idea Generation Due to the max output length limit of the LLM API, we first generate a large\\nnumber of shorter seed ideas. We keep the seed ideas short so that we can explore more different\\nideas given the same output token budget. To maximize diversity, we apply retrieval augmentation\\nhalf of the time when generating seed ideas, and we randomly select k= 10 papers from the top 20\\nretrieved papers when applying retrieval augmentation. Idea Filtering After expanding seed ideas into full project proposals, we did some basic filtering to\\nremove any project proposals that failed the novelty and feasibility checks:\\n1.Novelty: We use the literature review module to retrieve the top 10 most relevant papers to the\\ngenerated idea and ask the LLM to compare each of them to the generated idea. The idea will be\\nfiltered as long as any one of the retrieved papers is judged as equivalent. We keep the seed ideas short so that we can explore more different\\nideas given the same output token budget. We provide a demonstration example of the seed idea in\\nAppendix G. Then, we perform duplication and expand each remaining seed idea into a full project\\nproposal following our standard template in Appendix B.\\nRetrieval Augmentation We apply retrieval augmentation to the idea generation prompt in order to\\nincrease diversity in the idea generation. To maximize diversity, we apply retrieval augmentation\\nhalf of the time when generating seed ideas, and we randomly select k= 10 papers from the top 20\\nretrieved papers when applying retrieval augmentation. We provide a demonstration example of the seed idea in\\nAppendix G. Then, we perform duplication and expand each remaining seed idea into a full project\\nproposal following our standard template in Appendix B.\\nRetrieval Augmentation We apply retrieval augmentation to the idea generation prompt in order to\\nincrease diversity in the idea generation. To maximize diversity, we apply retrieval augmentation\\nhalf of the time when generating seed ideas, and we randomly select k= 10 papers from the top 20\\nretrieved papers when applying retrieval augmentation. Idea Filtering After expanding seed ideas into full project proposals, we did some basic filtering to\\nremove any project proposals that failed the novelty and feasibility checks:\\n1.Novelty: We use the literature review module to retrieve the top 10 most relevant papers to the\\ngenerated idea and ask the LLM to compare each of them to the generated idea. The idea will also be filtered if it involves any\\ninconsistency in the experimental setups or assumptions. For example, if the idea assumes\\nonly black-box API access of the LLMs, then it shouldn’t involve experiments that need internal\\nweight access. This filtered out about 1% of the generated project proposals.', 'metadata': {'page_label': '36', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"main result of the paper can llm generate novltive ideas\"\n",
    "\n",
    "text_qa_template = \"\"\"\n",
    "Context Information:\n",
    "--------------------\n",
    "{context_str}\n",
    "--------------------\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the context information and the query.\n",
    "2. Think through the problem step by step.\n",
    "3. Provide a concise and accurate answer based on the given context.\n",
    "4. If the answer cannot be determined from the context, state \"Based on the given information, I cannot provide a definitive answer.\"\n",
    "5. If you need to make any assumptions, clearly state them.\n",
    "6. If relevant, provide a brief explanation of your reasoning.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "response = ns.start_query(text_qa_template = text_qa_template, query = query )\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "print(f\"Response: {response['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ns.initialize_stream_chat` first. You can then print token by token in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 15:03:50,409 - nexusync.core.chat_engine - INFO - Chat engine initialized\n"
     ]
    }
   ],
   "source": [
    "text_qa_template = \"\"\"\n",
    "Context Information:\n",
    "--------------------\n",
    "{context_str}\n",
    "--------------------\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the context information and the query.\n",
    "2. Think through the problem step by step.\n",
    "3. Provide a concise and accurate answer based on the given context.\n",
    "4. If the answer cannot be determined from the context, state \"Based on the given information, I cannot provide a definitive answer.\"\n",
    "5. If you need to make any assumptions, clearly state them.\n",
    "6. If relevant, provide a brief explanation of your reasoning.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "ns.initialize_stream_chat(\n",
    "    text_qa_template=text_qa_template,\n",
    "    chat_mode=\"context\",\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main result of the paper \"Can LLMs Generate Novel Research Ideas?\" indicates that AI-generated ideas are judged to be more novel than human-generated ideas, based on a large-scale evaluation involving nearly 300 reviews. However, while these AI-generated ideas may appear more exciting, there are concerns about their feasibility and the potential decline in the quality of academic discourse due to the reliance on AI for idea generation. The study highlights the need for rigorous standards and accountability in evaluating both AI-assisted and human-generated research to ensure the integrity of academic contributions. Additionally, the paper raises issues related to intellectual credit, idea homogenization, and the importance of human collaboration in the ideation process.\n",
      "\n",
      "\n",
      "Metadata: {'sources': [{'source_text': 'page_label: 19\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nQuestion 2: Are evaluations based solely on ideas subjective? In this current study, we focused\\nsolely on evaluating the ideas themselves. Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Traditional frameworks for attributing credit in research,\\nbased on human authorship and contribution, become less clear when AI plays a significant role\\n19 The growing use of AI to generate research ideas raises serious concerns about the\\npotential abuse of these technologies by students or researchers who may flood academic conferences\\nwith low-quality or poorly thought-out submissions. The availability of LLM-generated content\\ncould lead to a decline in the overall quality of academic discourse, as some individuals might take a\\nlazy approach, relying on AI to both generate ideas and review submissions. This would undermine\\nthe credibility and integrity of the review process. Apart from speeding up scientific discovery, one could also imagine using such\\nexecution agents to automatically verify experiment results in existing papers or new submissions. We have also explored building an LLM agent to generate code to implement the generated ideas. Specifically, we provide a template codebase that consists of: (1) loading datasets from Huggingface\\nor generating synthetic test examples; (2) implementing baseline methods; (3) implementing the\\nproposed method; (3) loading or implementing the evaluation metrics; (4) running experiments on\\nthe testset with the baselines and the proposed method, so that the output of the agent will be a report\\nof the baseline performance as well as the proposed method’s performance. In this current study, we focused\\nsolely on evaluating the ideas themselves. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. We view the current study as a preliminary evaluation of AI-generated ideas. 11 Ethical Considerations\\nPublication Policy. The growing use of AI to generate research ideas raises serious concerns about the\\npotential abuse of these technologies by students or researchers who may flood academic conferences\\nwith low-quality or poorly thought-out submissions. The availability of LLM-generated content\\ncould lead to a decline in the overall quality of academic discourse, as some individuals might take a\\nlazy approach, relying on AI to both generate ideas and review submissions. Question 3: Why do you focus only on prompting-based research in NLP? The scope of our study\\nis limited to prompting research ideas within NLP . We chose this design to facilitate the next phase\\nof our execution experiment, where we prefer research ideas that are less resource-demanding and\\ncan be executed relatively quickly. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. We view the current study as a preliminary evaluation of AI-generated ideas. In the next phase, we\\nwill recruit researchers to execute some AI and human-generated ideas into full projects. page_label: 19\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nQuestion 2: Are evaluations based solely on ideas subjective? In this current study, we focused\\nsolely on evaluating the ideas themselves. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. To prevent this, it is essential to\\nhold researchers accountable for the outputs generated through AI tools. Rigorous standards must be\\napplied equally to both AI-assisted and human-generated research to ensure that the use of LLMs\\ndoes not result in misleading, superficial, or unethical academic contributions. Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Traditional frameworks for attributing credit in research,\\nbased on human authorship and contribution, become less clear when AI plays a significant role\\n19 We believe that the evaluation protocols we established should be\\napplicable to other research domains as well, although the conclusions could be different depending\\non the research fields. Future work should consider extending such human study to other research\\ndomains and it would be interesting to compare how the conclusions differ. Question 4: Can you automate idea execution as well? Question 4: Can you automate idea execution as well? It is tempting to envision an end-to-end\\nautomated research pipeline where AI agents can implement AI-generated ideas to directly evaluate\\ntheir effectiveness. Apart from speeding up scientific discovery, one could also imagine using such\\nexecution agents to automatically verify experiment results in existing papers or new submissions. Rigorous standards must be\\napplied equally to both AI-assisted and human-generated research to ensure that the use of LLMs\\ndoes not result in misleading, superficial, or unethical academic contributions. Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Specifically, we provide a template codebase that consists of: (1) loading datasets from Huggingface\\nor generating synthetic test examples; (2) implementing baseline methods; (3) implementing the\\nproposed method; (3) loading or implementing the evaluation metrics; (4) running experiments on\\nthe testset with the baselines and the proposed method, so that the output of the agent will be a report\\nof the baseline performance as well as the proposed method’s performance. While this agent can\\ngenerate code that compiles and executes, we find that the automated experiments can be misleading\\nbecause the agent often skips or modifies steps in the baselines or proposed methods. In some cases,\\nthe metric functions are also not correctly defined. The risks are real. Without proper oversight, we\\ncould see a deluge of submissions that lack depth or intellectual merit. To prevent this, it is essential to\\nhold researchers accountable for the outputs generated through AI tools. We view the current study as a preliminary evaluation of AI-generated ideas. In the next phase, we\\nwill recruit researchers to execute some AI and human-generated ideas into full projects. This will\\nenable reviewers to assess the complete experimental outcomes, providing a more reliable basis for\\nevaluation. Future work should consider extending such human study to other research\\ndomains and it would be interesting to compare how the conclusions differ. Question 4: Can you automate idea execution as well? It is tempting to envision an end-to-end\\nautomated research pipeline where AI agents can implement AI-generated ideas to directly evaluate\\ntheir effectiveness. Furthermore, it will allow us to analyze whether our initial idea evaluations align with\\nthe assessments of the actual project outcomes. Question 3: Why do you focus only on prompting-based research in NLP? The scope of our study\\nis limited to prompting research ideas within NLP . It is tempting to envision an end-to-end\\nautomated research pipeline where AI agents can implement AI-generated ideas to directly evaluate\\ntheir effectiveness. Apart from speeding up scientific discovery, one could also imagine using such\\nexecution agents to automatically verify experiment results in existing papers or new submissions. We have also explored building an LLM agent to generate code to implement the generated ideas. We chose this design to facilitate the next phase\\nof our execution experiment, where we prefer research ideas that are less resource-demanding and\\ncan be executed relatively quickly. We believe that the evaluation protocols we established should be\\napplicable to other research domains as well, although the conclusions could be different depending\\non the research fields. Future work should consider extending such human study to other research\\ndomains and it would be interesting to compare how the conclusions differ. This will\\nenable reviewers to assess the complete experimental outcomes, providing a more reliable basis for\\nevaluation. Furthermore, it will allow us to analyze whether our initial idea evaluations align with\\nthe assessments of the actual project outcomes. Question 3: Why do you focus only on prompting-based research in NLP? Without proper oversight, we\\ncould see a deluge of submissions that lack depth or intellectual merit. To prevent this, it is essential to\\nhold researchers accountable for the outputs generated through AI tools. Rigorous standards must be\\napplied equally to both AI-assisted and human-generated research to ensure that the use of LLMs\\ndoes not result in misleading, superficial, or unethical academic contributions.', 'metadata': {'page_label': '19', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}, {'source_text': 'page_label: 20\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nin idea generation. Questions arise around how to distribute credit between the developers of the\\nLLM, the researchers who designed the frameworks for its use, and the researchers who integrate\\nAI-generated ideas into their work. Idea Homogenization. Our analysis showed that current LLMs lack diversity in idea generation. This raises important concerns that wide adoption of LLMs can result in idea homogenization, where\\nthe generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time,\\nthis could lead to a reduction in the richness and diversity of research outputs globally. Future work\\nshould develop ways to either improve LLMs themselves or refine our idea generation methods to\\npromote idea diversity. It’s also important to note that our evaluation primarily assesses the quality of\\nthe typical ideas being generated, and may not fully capture the long tail of unique or novel ideas that\\nwould be truly transformative. Our analysis showed that current LLMs lack diversity in idea generation. This raises important concerns that wide adoption of LLMs can result in idea homogenization, where\\nthe generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time,\\nthis could lead to a reduction in the richness and diversity of research outputs globally. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences. Overreliance on AI could lead to a decline in original human thought, while the increasing use of\\nLLMs for ideation might reduce opportunities for human collaboration, which is essential for refining\\nand expanding ideas. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. page_label: 20\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nin idea generation. Questions arise around how to distribute credit between the developers of the\\nLLM, the researchers who designed the frameworks for its use, and the researchers who integrate\\nAI-generated ideas into their work. Furthermore, it becomes increasingly difficult to trace the origins\\nof AI-generated contributions, especially when they draw from vast datasets composed of numerous\\nsources. Future work\\nshould develop ways to either improve LLMs themselves or refine our idea generation methods to\\npromote idea diversity. It’s also important to note that our evaluation primarily assesses the quality of\\nthe typical ideas being generated, and may not fully capture the long tail of unique or novel ideas that\\nwould be truly transformative. Impact on Human Researchers. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. Understanding how to\\nintegrate LLMs into this collaborative process without disrupting the social fabric of research will\\nbe an important ongoing problem, requiring careful consideration of the broader sociotechnical\\nimplications. 20 Impact on Human Researchers. The integration of AI into research idea generation introduces a\\ncomplex sociotechnical challenge, as research is fundamentally a community-driven, collaborative\\neffort. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences. Potential for Misuse. AI-generated research ideas, especially those that introduce novel concepts,\\nhave the potential to be misused in ways that could lead to harmful or destabilizing outcomes. For\\ninstance, ideation agents could be leveraged to generate adversarial attack strategies or other unethical\\napplications. Overreliance on AI could lead to a decline in original human thought, while the increasing use of\\nLLMs for ideation might reduce opportunities for human collaboration, which is essential for refining\\nand expanding ideas. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. This complexity calls for a broader rethinking of how intellectual credit is assigned in\\nAI-driven research. While a complete overhaul of legal and academic norms is beyond the scope\\nof this project, we advocate for the adoption of transparent documentation practices. Researchers\\nshould clearly disclose the role AI played in the idea generation process, specifying which models,\\ndata sources, and frameworks were used, and outlining the level of human involvement. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. Understanding how to\\nintegrate LLMs into this collaborative process without disrupting the social fabric of research will\\nbe an important ongoing problem, requiring careful consideration of the broader sociotechnical\\nimplications. Furthermore, it becomes increasingly difficult to trace the origins\\nof AI-generated contributions, especially when they draw from vast datasets composed of numerous\\nsources. This complexity calls for a broader rethinking of how intellectual credit is assigned in\\nAI-driven research. While a complete overhaul of legal and academic norms is beyond the scope\\nof this project, we advocate for the adoption of transparent documentation practices. Additionally, we believe it would be meaningful to create\\nsafety benchmarks that assess the ethical and safe application of AI-generated ideas. Idea Homogenization. Our analysis showed that current LLMs lack diversity in idea generation. The integration of AI into research idea generation introduces a\\ncomplex sociotechnical challenge, as research is fundamentally a community-driven, collaborative\\neffort. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences. Overreliance on AI could lead to a decline in original human thought, while the increasing use of\\nLLMs for ideation might reduce opportunities for human collaboration, which is essential for refining\\nand expanding ideas. This raises important concerns that wide adoption of LLMs can result in idea homogenization, where\\nthe generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time,\\nthis could lead to a reduction in the richness and diversity of research outputs globally. Future work\\nshould develop ways to either improve LLMs themselves or refine our idea generation methods to\\npromote idea diversity. While a complete overhaul of legal and academic norms is beyond the scope\\nof this project, we advocate for the adoption of transparent documentation practices. Researchers\\nshould clearly disclose the role AI played in the idea generation process, specifying which models,\\ndata sources, and frameworks were used, and outlining the level of human involvement. This could\\nensure that the credit distribution in AI-supported research is as transparent and fair as possible. Our stance is that such discussions on safety\\nshould be evidence-based to the extent that it is possible, and careful evaluation work is an important\\ncomponent of keeping these discussions grounded in actual, measured capabilities of these systems. We advocate for continued safety research specifically targeting these types of concerns—such as the\\ndevelopment of Reinforcement Learning from Human Feedback (RLHF) systems or anti-jailbreak\\nmechanisms for research ideation agents. Additionally, we believe it would be meaningful to create\\nsafety benchmarks that assess the ethical and safe application of AI-generated ideas.', 'metadata': {'page_label': '20', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}, {'source_text': 'page_label: 3\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nOur evaluation-centric approach complements many recent methods-centric works that attempt to\\ninstantiate research agents (Baek et al., 2024, Li et al., 2024, Lu et al., 2024, Wang et al., 2024, Yang et al.,\\n2024). The majority of these works rely on fast and lower-cost evaluation surrogates – either by decreas-\\ning the number of expert reviewers (Baek et al., 2024, Li et al., 2024, Wang et al., 2024, Yang et al., 2024),\\nconstraining the length and detailedness of the ideas (Wang et al., 2024, Yang et al., 2024), or relying on\\nLLM-as-a-judge (Lu et al., 2024). Qualitative analysis of free-text responses in our review corroborates these findings on novelty and\\nfeasibility. Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators. 2 Problem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. We further structure our ideation process to avoid selection-bias-based confounders in ideation. If\\nwe simply ask LLMs and humans to produce ideas on ‘prompting topics’, we may find that LLMs\\nand humans differ in the types of research ideas they produce (for example, LLMs may naturally\\nsuggest more projects on safer topics, which might be judged as less exciting by humans). This would\\n3 Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators. 2 Problem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. While\\nthis goal is simple, there is no existing consensus on how to formulate the task of research ideation\\nand evaluation, and we begin by defining the key aspects of our experiment design. Our work takes the opposite approach,\\nperforming a year-long and high-cost evaluation that provides human expert baselines and a stan-\\ndardized evaluation protocol to serve as a foundation for future follow-up studies and methods work. Through nearly 300 reviews across all our conditions, we find that AI-generated ideas are judged\\nas more novel than human expert ideas ( p<0.05), which holds robustly under multiple hypothesis\\ncorrection and across different statistical tests. We find some signs that these gains are correlated\\nwith excitement and overall score, and may come at the slight expense of feasibility, but our study\\nsize did not have sufficient power to conclusively identify these effects (Figure 2). page_label: 3\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nOur evaluation-centric approach complements many recent methods-centric works that attempt to\\ninstantiate research agents (Baek et al., 2024, Li et al., 2024, Lu et al., 2024, Wang et al., 2024, Yang et al.,\\n2024). The majority of these works rely on fast and lower-cost evaluation surrogates – either by decreas-\\ning the number of expert reviewers (Baek et al., 2024, Li et al., 2024, Wang et al., 2024, Yang et al., 2024),\\nconstraining the length and detailedness of the ideas (Wang et al., 2024, Yang et al., 2024), or relying on\\nLLM-as-a-judge (Lu et al., 2024). They do not perform the large-scale human comparison studies that\\nare needed to answer the motivating question of our work. We find some signs that these gains are correlated\\nwith excitement and overall score, and may come at the slight expense of feasibility, but our study\\nsize did not have sufficient power to conclusively identify these effects (Figure 2). Qualitative analysis of free-text responses in our review corroborates these findings on novelty and\\nfeasibility. Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators. They can be simple\\ntricks to improve model performance, or they may be large-scale research programs that form the\\nbasis of a Ph.D. thesis. Any experiment on ideation must carefully balance the realisticness and\\ninterestingness of a research idea with the practical realities of eliciting ideas from a large population. thesis. Any experiment on ideation must carefully balance the realisticness and\\ninterestingness of a research idea with the practical realities of eliciting ideas from a large population. In our case, these tradeoffs are even more pronounced, as we have designed our ideation experiments\\nso that the resulting ideas can be executed by experts in a follow-up set of experiments. 2 Problem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. While\\nthis goal is simple, there is no existing consensus on how to formulate the task of research ideation\\nand evaluation, and we begin by defining the key aspects of our experiment design. We think of research idea evaluation as consisting of three separate components: 1). We outline our experiment design in each of these three parts\\nwith particular focus on potential confounders, such as the area of research, the format of a research\\nidea, and the evaluation process. Ideation Scope and Instructions Research ideas can take many different forms. They can be simple\\ntricks to improve model performance, or they may be large-scale research programs that form the\\nbasis of a Ph.D. This class\\nof projects strikes a reasonable trade-off among our constraints. The most impactful prompting\\nprojects like chain-of-thought have had a major influence on LLM performance (Wei et al., 2022), and\\nprompting projects are executable with minimal computing hardware. We further structure our ideation process to avoid selection-bias-based confounders in ideation. We think of research idea evaluation as consisting of three separate components: 1). the idea itself,\\ngenerated in response to our instructions, 2). the writeup which communicates the idea, and 3). Ideation Scope and Instructions Research ideas can take many different forms. They can be simple\\ntricks to improve model performance, or they may be large-scale research programs that form the\\nbasis of a Ph.D. thesis. the\\nevaluation of the writeup by experts. We outline our experiment design in each of these three parts\\nwith particular focus on potential confounders, such as the area of research, the format of a research\\nidea, and the evaluation process. Ideation Scope and Instructions Research ideas can take many different forms. the idea itself,\\ngenerated in response to our instructions, 2). the writeup which communicates the idea, and 3). the\\nevaluation of the writeup by experts. Any experiment on ideation must carefully balance the realisticness and\\ninterestingness of a research idea with the practical realities of eliciting ideas from a large population. In our case, these tradeoffs are even more pronounced, as we have designed our ideation experiments\\nso that the resulting ideas can be executed by experts in a follow-up set of experiments. These constraints have led us to study prompting-based NLP research as a testbed for our study. While\\nthis goal is simple, there is no existing consensus on how to formulate the task of research ideation\\nand evaluation, and we begin by defining the key aspects of our experiment design. We think of research idea evaluation as consisting of three separate components: 1). the idea itself,\\ngenerated in response to our instructions, 2).', 'metadata': {'page_label': '3', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"main result of the paper can llm generate novltive ideas\"\n",
    "\n",
    "\n",
    "for item in ns.start_chat_stream(query):\n",
    "    if isinstance(item, str):\n",
    "        # This is a token, print or process as needed\n",
    "        print(item, end='', flush=True)\n",
    "    else:\n",
    "        # This is the final response with metadata\n",
    "        # print(\"\\n\\nFull response:\", item['response'])\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"Metadata:\", item['metadata'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild the index when either of the following is changed:\n",
    "- openai_model_yn\n",
    "- embedding_model\n",
    "- language_model\n",
    "- base_url\n",
    "- chroma_db_dir\n",
    "- index_persist_dir\n",
    "- chroma_collection_name\n",
    "- chunk_overlap\n",
    "- chunk_size\n",
    "- recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we rebuilt it with OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zy-wsl/miniconda3/envs/nexusync/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-20 15:02:57,850 - nexusync.core.indexing_functions - INFO - Starting index rebuild process...\n",
      "2024-10-20 15:02:57,940 - nexusync.utils.embedding_models.set_embedding_model - INFO - Using OpenAI embedding model: text-embedding-3-large\n",
      "2024-10-20 15:02:57,944 - nexusync.utils.embedding_models.set_language_model - INFO - Using OpenAI LLM model: gpt-4o-mini\n",
      "2024-10-20 15:02:57,946 - nexusync.core.indexing_functions - INFO - Deleting existing index directory: index_storage\n",
      "2024-10-20 15:02:57,952 - nexusync.core.indexing_functions - WARNING - Chroma DB directory chroma_db does not exist. Skipping deletion.\n",
      "2024-10-20 15:02:57,953 - nexusync.core.indexing_functions - WARNING - Index not found. Building a new index.\n",
      "2024-10-20 15:02:59,738 - nexusync.core.indexing_functions - INFO - Loaded 2 files from all directories.\n",
      "2024-10-20 15:03:06,072 - nexusync.core.indexing_functions - INFO - Index Built.\n"
     ]
    }
   ],
   "source": [
    "from nexusync import rebuild_index\n",
    "from nexusync import NexuSync\n",
    "\n",
    "OPENAI_MODEL_YN = True # if False, you will use ollama model\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\" # suggested embedding model\n",
    "LANGUAGE_MODEL = 'gpt-4o-mini' # you need to download ollama model first, please check https://ollama.com/download\n",
    "TEMPERATURE = 0.4 # range from 0 to 1, higher means higher creativitiy level\n",
    "CHROMA_DB_DIR = 'chroma_db'\n",
    "INDEX_PERSIST_DIR = 'index_storage'\n",
    "CHROMA_COLLECTION_NAME = 'my_collection'\n",
    "INPUT_DIRS = [\"../sample_docs\"] # can specify multiple document paths\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 20\n",
    "RECURSIVE = True\n",
    "\n",
    "rebuild_index(input_dirs=INPUT_DIRS, \n",
    "              openai_model_yn=OPENAI_MODEL_YN, \n",
    "              embedding_model=EMBEDDING_MODEL, \n",
    "              language_model=LANGUAGE_MODEL, \n",
    "              temperature=TEMPERATURE, \n",
    "              chroma_db_dir = CHROMA_DB_DIR,\n",
    "              index_persist_dir = INDEX_PERSIST_DIR,\n",
    "              chroma_collection_name=CHROMA_COLLECTION_NAME,\n",
    "              chunk_overlap=CHUNK_OVERLAP,\n",
    "              chunk_size=CHUNK_SIZE,\n",
    "              recursive=RECURSIVE\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 15:03:06,089 - nexusync.utils.embedding_models.set_embedding_model - INFO - Using OpenAI embedding model: text-embedding-3-large\n",
      "2024-10-20 15:03:06,096 - nexusync.utils.embedding_models.set_language_model - INFO - Using OpenAI LLM model: gpt-4o-mini\n",
      "2024-10-20 15:03:06,096 - nexusync.NexuSync - INFO - Vectors and Querier initialized successfully.\n",
      "2024-10-20 15:03:07,413 - nexusync.core.indexer - INFO - Index already built. Loading from disk.\n"
     ]
    }
   ],
   "source": [
    "# reinitiate the ns after rebuilding the index\n",
    "ns = NexuSync(input_dirs=INPUT_DIRS, \n",
    "              openai_model_yn=OPENAI_MODEL_YN, \n",
    "              embedding_model=EMBEDDING_MODEL, \n",
    "              language_model=LANGUAGE_MODEL, \n",
    "              temperature=TEMPERATURE, \n",
    "              chroma_db_dir = CHROMA_DB_DIR,\n",
    "              index_persist_dir = INDEX_PERSIST_DIR,\n",
    "              chroma_collection_name=CHROMA_COLLECTION_NAME,\n",
    "              chunk_overlap=CHUNK_OVERLAP,\n",
    "              chunk_size=CHUNK_SIZE,\n",
    "              recursive=RECURSIVE\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 14:59:40,277 - nexusync.NexuSync - INFO - Starting query: main result of the paper can llm generate novltive ideas\n",
      "2024-10-20 14:59:47,608 - nexusync.NexuSync - INFO - Query completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: main result of the paper can llm generate novltive ideas\n",
      "Response: The main result of the paper indicates that AI-generated ideas are judged as more novel than human expert ideas, with statistical significance (p<0.05). However, this novelty may come at the slight expense of feasibility, although the study size did not have sufficient power to conclusively identify these effects. The paper emphasizes that while LLMs can generate novel ideas, there are concerns regarding the quality and diversity of these ideas, as well as potential issues related to intellectual credit and the overall impact on academic discourse.\n",
      "Response: {'sources': [{'source_text': 'page_label: 19\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nQuestion 2: Are evaluations based solely on ideas subjective? In this current study, we focused\\nsolely on evaluating the ideas themselves. Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Traditional frameworks for attributing credit in research,\\nbased on human authorship and contribution, become less clear when AI plays a significant role\\n19 The growing use of AI to generate research ideas raises serious concerns about the\\npotential abuse of these technologies by students or researchers who may flood academic conferences\\nwith low-quality or poorly thought-out submissions. The availability of LLM-generated content\\ncould lead to a decline in the overall quality of academic discourse, as some individuals might take a\\nlazy approach, relying on AI to both generate ideas and review submissions. This would undermine\\nthe credibility and integrity of the review process. Apart from speeding up scientific discovery, one could also imagine using such\\nexecution agents to automatically verify experiment results in existing papers or new submissions. We have also explored building an LLM agent to generate code to implement the generated ideas. Specifically, we provide a template codebase that consists of: (1) loading datasets from Huggingface\\nor generating synthetic test examples; (2) implementing baseline methods; (3) implementing the\\nproposed method; (3) loading or implementing the evaluation metrics; (4) running experiments on\\nthe testset with the baselines and the proposed method, so that the output of the agent will be a report\\nof the baseline performance as well as the proposed method’s performance. In this current study, we focused\\nsolely on evaluating the ideas themselves. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. We view the current study as a preliminary evaluation of AI-generated ideas. 11 Ethical Considerations\\nPublication Policy. The growing use of AI to generate research ideas raises serious concerns about the\\npotential abuse of these technologies by students or researchers who may flood academic conferences\\nwith low-quality or poorly thought-out submissions. The availability of LLM-generated content\\ncould lead to a decline in the overall quality of academic discourse, as some individuals might take a\\nlazy approach, relying on AI to both generate ideas and review submissions. Question 3: Why do you focus only on prompting-based research in NLP? The scope of our study\\nis limited to prompting research ideas within NLP . We chose this design to facilitate the next phase\\nof our execution experiment, where we prefer research ideas that are less resource-demanding and\\ncan be executed relatively quickly. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. We view the current study as a preliminary evaluation of AI-generated ideas. In the next phase, we\\nwill recruit researchers to execute some AI and human-generated ideas into full projects. page_label: 19\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nQuestion 2: Are evaluations based solely on ideas subjective? In this current study, we focused\\nsolely on evaluating the ideas themselves. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. To prevent this, it is essential to\\nhold researchers accountable for the outputs generated through AI tools. Rigorous standards must be\\napplied equally to both AI-assisted and human-generated research to ensure that the use of LLMs\\ndoes not result in misleading, superficial, or unethical academic contributions. Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Traditional frameworks for attributing credit in research,\\nbased on human authorship and contribution, become less clear when AI plays a significant role\\n19 We believe that the evaluation protocols we established should be\\napplicable to other research domains as well, although the conclusions could be different depending\\non the research fields. Future work should consider extending such human study to other research\\ndomains and it would be interesting to compare how the conclusions differ. Question 4: Can you automate idea execution as well? Question 4: Can you automate idea execution as well? It is tempting to envision an end-to-end\\nautomated research pipeline where AI agents can implement AI-generated ideas to directly evaluate\\ntheir effectiveness. Apart from speeding up scientific discovery, one could also imagine using such\\nexecution agents to automatically verify experiment results in existing papers or new submissions. Rigorous standards must be\\napplied equally to both AI-assisted and human-generated research to ensure that the use of LLMs\\ndoes not result in misleading, superficial, or unethical academic contributions. Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Specifically, we provide a template codebase that consists of: (1) loading datasets from Huggingface\\nor generating synthetic test examples; (2) implementing baseline methods; (3) implementing the\\nproposed method; (3) loading or implementing the evaluation metrics; (4) running experiments on\\nthe testset with the baselines and the proposed method, so that the output of the agent will be a report\\nof the baseline performance as well as the proposed method’s performance. While this agent can\\ngenerate code that compiles and executes, we find that the automated experiments can be misleading\\nbecause the agent often skips or modifies steps in the baselines or proposed methods. In some cases,\\nthe metric functions are also not correctly defined. The risks are real. Without proper oversight, we\\ncould see a deluge of submissions that lack depth or intellectual merit. To prevent this, it is essential to\\nhold researchers accountable for the outputs generated through AI tools.', 'metadata': {'page_label': '19', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}, {'source_text': 'page_label: 20\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nin idea generation. Questions arise around how to distribute credit between the developers of the\\nLLM, the researchers who designed the frameworks for its use, and the researchers who integrate\\nAI-generated ideas into their work. Idea Homogenization. Our analysis showed that current LLMs lack diversity in idea generation. This raises important concerns that wide adoption of LLMs can result in idea homogenization, where\\nthe generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time,\\nthis could lead to a reduction in the richness and diversity of research outputs globally. Future work\\nshould develop ways to either improve LLMs themselves or refine our idea generation methods to\\npromote idea diversity. It’s also important to note that our evaluation primarily assesses the quality of\\nthe typical ideas being generated, and may not fully capture the long tail of unique or novel ideas that\\nwould be truly transformative. Our analysis showed that current LLMs lack diversity in idea generation. This raises important concerns that wide adoption of LLMs can result in idea homogenization, where\\nthe generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time,\\nthis could lead to a reduction in the richness and diversity of research outputs globally. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences. Overreliance on AI could lead to a decline in original human thought, while the increasing use of\\nLLMs for ideation might reduce opportunities for human collaboration, which is essential for refining\\nand expanding ideas. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. page_label: 20\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nin idea generation. Questions arise around how to distribute credit between the developers of the\\nLLM, the researchers who designed the frameworks for its use, and the researchers who integrate\\nAI-generated ideas into their work. Furthermore, it becomes increasingly difficult to trace the origins\\nof AI-generated contributions, especially when they draw from vast datasets composed of numerous\\nsources. Future work\\nshould develop ways to either improve LLMs themselves or refine our idea generation methods to\\npromote idea diversity. It’s also important to note that our evaluation primarily assesses the quality of\\nthe typical ideas being generated, and may not fully capture the long tail of unique or novel ideas that\\nwould be truly transformative. Impact on Human Researchers. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. Understanding how to\\nintegrate LLMs into this collaborative process without disrupting the social fabric of research will\\nbe an important ongoing problem, requiring careful consideration of the broader sociotechnical\\nimplications. 20 Impact on Human Researchers. The integration of AI into research idea generation introduces a\\ncomplex sociotechnical challenge, as research is fundamentally a community-driven, collaborative\\neffort. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences. Potential for Misuse. AI-generated research ideas, especially those that introduce novel concepts,\\nhave the potential to be misused in ways that could lead to harmful or destabilizing outcomes. For\\ninstance, ideation agents could be leveraged to generate adversarial attack strategies or other unethical\\napplications. Overreliance on AI could lead to a decline in original human thought, while the increasing use of\\nLLMs for ideation might reduce opportunities for human collaboration, which is essential for refining\\nand expanding ideas. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. This complexity calls for a broader rethinking of how intellectual credit is assigned in\\nAI-driven research. While a complete overhaul of legal and academic norms is beyond the scope\\nof this project, we advocate for the adoption of transparent documentation practices. Researchers\\nshould clearly disclose the role AI played in the idea generation process, specifying which models,\\ndata sources, and frameworks were used, and outlining the level of human involvement. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. Understanding how to\\nintegrate LLMs into this collaborative process without disrupting the social fabric of research will\\nbe an important ongoing problem, requiring careful consideration of the broader sociotechnical\\nimplications. Furthermore, it becomes increasingly difficult to trace the origins\\nof AI-generated contributions, especially when they draw from vast datasets composed of numerous\\nsources. This complexity calls for a broader rethinking of how intellectual credit is assigned in\\nAI-driven research. While a complete overhaul of legal and academic norms is beyond the scope\\nof this project, we advocate for the adoption of transparent documentation practices.', 'metadata': {'page_label': '20', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}, {'source_text': 'page_label: 3\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nOur evaluation-centric approach complements many recent methods-centric works that attempt to\\ninstantiate research agents (Baek et al., 2024, Li et al., 2024, Lu et al., 2024, Wang et al., 2024, Yang et al.,\\n2024). The majority of these works rely on fast and lower-cost evaluation surrogates – either by decreas-\\ning the number of expert reviewers (Baek et al., 2024, Li et al., 2024, Wang et al., 2024, Yang et al., 2024),\\nconstraining the length and detailedness of the ideas (Wang et al., 2024, Yang et al., 2024), or relying on\\nLLM-as-a-judge (Lu et al., 2024). Qualitative analysis of free-text responses in our review corroborates these findings on novelty and\\nfeasibility. Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators. 2 Problem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. We further structure our ideation process to avoid selection-bias-based confounders in ideation. If\\nwe simply ask LLMs and humans to produce ideas on ‘prompting topics’, we may find that LLMs\\nand humans differ in the types of research ideas they produce (for example, LLMs may naturally\\nsuggest more projects on safer topics, which might be judged as less exciting by humans). This would\\n3 Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators. 2 Problem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. While\\nthis goal is simple, there is no existing consensus on how to formulate the task of research ideation\\nand evaluation, and we begin by defining the key aspects of our experiment design. Our work takes the opposite approach,\\nperforming a year-long and high-cost evaluation that provides human expert baselines and a stan-\\ndardized evaluation protocol to serve as a foundation for future follow-up studies and methods work. Through nearly 300 reviews across all our conditions, we find that AI-generated ideas are judged\\nas more novel than human expert ideas ( p<0.05), which holds robustly under multiple hypothesis\\ncorrection and across different statistical tests. We find some signs that these gains are correlated\\nwith excitement and overall score, and may come at the slight expense of feasibility, but our study\\nsize did not have sufficient power to conclusively identify these effects (Figure 2). page_label: 3\\nfile_path: /mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf\\n\\nOur evaluation-centric approach complements many recent methods-centric works that attempt to\\ninstantiate research agents (Baek et al., 2024, Li et al., 2024, Lu et al., 2024, Wang et al., 2024, Yang et al.,\\n2024). The majority of these works rely on fast and lower-cost evaluation surrogates – either by decreas-\\ning the number of expert reviewers (Baek et al., 2024, Li et al., 2024, Wang et al., 2024, Yang et al., 2024),\\nconstraining the length and detailedness of the ideas (Wang et al., 2024, Yang et al., 2024), or relying on\\nLLM-as-a-judge (Lu et al., 2024). They do not perform the large-scale human comparison studies that\\nare needed to answer the motivating question of our work. We find some signs that these gains are correlated\\nwith excitement and overall score, and may come at the slight expense of feasibility, but our study\\nsize did not have sufficient power to conclusively identify these effects (Figure 2). Qualitative analysis of free-text responses in our review corroborates these findings on novelty and\\nfeasibility. Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators. They can be simple\\ntricks to improve model performance, or they may be large-scale research programs that form the\\nbasis of a Ph.D. thesis. Any experiment on ideation must carefully balance the realisticness and\\ninterestingness of a research idea with the practical realities of eliciting ideas from a large population. thesis. Any experiment on ideation must carefully balance the realisticness and\\ninterestingness of a research idea with the practical realities of eliciting ideas from a large population. In our case, these tradeoffs are even more pronounced, as we have designed our ideation experiments\\nso that the resulting ideas can be executed by experts in a follow-up set of experiments. 2 Problem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. While\\nthis goal is simple, there is no existing consensus on how to formulate the task of research ideation\\nand evaluation, and we begin by defining the key aspects of our experiment design. We think of research idea evaluation as consisting of three separate components: 1). We outline our experiment design in each of these three parts\\nwith particular focus on potential confounders, such as the area of research, the format of a research\\nidea, and the evaluation process. Ideation Scope and Instructions Research ideas can take many different forms. They can be simple\\ntricks to improve model performance, or they may be large-scale research programs that form the\\nbasis of a Ph.D. This class\\nof projects strikes a reasonable trade-off among our constraints. The most impactful prompting\\nprojects like chain-of-thought have had a major influence on LLM performance (Wei et al., 2022), and\\nprompting projects are executable with minimal computing hardware. We further structure our ideation process to avoid selection-bias-based confounders in ideation. We think of research idea evaluation as consisting of three separate components: 1). the idea itself,\\ngenerated in response to our instructions, 2). the writeup which communicates the idea, and 3).', 'metadata': {'page_label': '3', 'file_name': 'Can LLMs Generate Novel Research Ideas.pdf', 'file_path': '/mnt/d/nexusync/notebooks/../sample_docs/Can LLMs Generate Novel Research Ideas.pdf', 'file_type': 'application/pdf', 'file_size': 942188, 'creation_date': '2024-10-20', 'last_modified_date': '2024-10-20'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Test the new built index\n",
    "query = \"main result of the paper can llm generate novltive ideas\"\n",
    "\n",
    "text_qa_template = \"\"\"\n",
    "Context Information:\n",
    "--------------------\n",
    "{context_str}\n",
    "--------------------\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the context information and the query.\n",
    "2. Think through the problem step by step.\n",
    "3. Provide a concise and accurate answer based on the given context.\n",
    "4. If the answer cannot be determined from the context, state \"Based on the given information, I cannot provide a definitive answer.\"\n",
    "5. If you need to make any assumptions, clearly state them.\n",
    "6. If relevant, provide a brief explanation of your reasoning.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "response = ns.start_query(text_qa_template = text_qa_template, query = query )\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "print(f\"Response: {response['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 15:03:24,569 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-20 15:03:24,572 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-20 15:03:26,569 - nexusync.core.indexer - INFO - Loaded 2 files from ../sample_docs\n",
      "2024-10-20 15:03:26,571 - nexusync.core.indexer - INFO - No files were modified or added in ../sample_docs\n",
      "2024-10-20 15:03:26,571 - nexusync.core.indexer - INFO - No files were modified or added in any directory\n",
      "2024-10-20 15:03:26,573 - nexusync.core.indexer - INFO - No deleted files found.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and persist the vector store in a chroma db\n",
    "ns.refresh_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat History will only be available when starting a stream chat. Will not capture a normal query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat History:\n",
      "Human: main result of the paper can llm generate novltive ideas\n",
      "AI: The main result of the paper \"Can LLMs Generate Novel Research Ideas?\" indicates that AI-generated ideas are judged to be more novel than human-generated ideas, based on a large-scale evaluation involving nearly 300 reviews. However, while these AI-generated ideas may appear more exciting, there are concerns about their feasibility and the potential decline in the quality of academic discourse due to the reliance on AI for idea generation. The study highlights the need for rigorous standards and accountability in evaluating both AI-assisted and human-generated research to ensure the integrity of academic contributions. Additionally, the paper raises issues related to intellectual credit, idea homogenization, and the importance of human collaboration in the ideation process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get chat history\n",
    "chat_history = ns.chat_engine.get_chat_history()\n",
    "print(\"Chat History:\")\n",
    "for entry in chat_history:\n",
    "    print(f\"Human: {entry['query']}\")\n",
    "    print(f\"AI: {entry['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 15:04:57,399 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-20 15:04:57,401 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-20 15:04:59,516 - nexusync.core.indexer - INFO - Loaded 2 files from ../sample_docs\n",
      "2024-10-20 15:04:59,829 - nexusync.core.indexer - INFO - Updated file: /mnt/d/nexusync/notebooks/../sample_docs/new_added.txt\n",
      "2024-10-20 15:04:59,830 - nexusync.core.indexer - INFO - Total files modified or added: 1\n",
      "2024-10-20 15:04:59,832 - nexusync.core.indexer - INFO - No deleted files found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add a new document\n",
    "with open(\"../sample_docs/new_added.txt\", \"w\") as f:\n",
    "    f.write(\"Breaking News: Trump and Harris had a fight!!!!\")\n",
    "\n",
    "# Refresh the index: incremental in new files and detect deleted files in the folder\n",
    "ns.refresh_index()\n",
    "print(\"Index refreshed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:40:27,349 - nexusync.core.indexer - INFO - Starting index refresh process...\n",
      "2024-10-07 15:40:27,353 - nexusync.core.indexer - INFO - Processing directory: ../sample_docs\n",
      "2024-10-07 15:40:36,886 - nexusync.core.indexer - INFO - Loaded 7 files from ../sample_docs\n",
      "2024-10-07 15:40:36,887 - nexusync.core.indexer - INFO - Updated 0 files in ../sample_docs\n",
      "2024-10-07 15:40:36,888 - nexusync.core.indexer - INFO - No deleted files found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed after deletion.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Delete the new document\n",
    "# os.remove('../sample_docs/Nvidia ecosystem.pptx')\n",
    "# print(\"New document deleted.\")\n",
    "\n",
    "ns.refresh_index()\n",
    "print(\"Index refreshed after deletion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexusync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
